<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>EP10-多机DeepSeek优化部署</title>
      <link href="/2025/05/18/FIXME-EP10/"/>
      <url>/2025/05/18/FIXME-EP10/</url>
      
        <content type="html"><![CDATA[<h1 id="FIXME-EP10-多机DeepSeek优化部署"><a href="#FIXME-EP10-多机DeepSeek优化部署" class="headerlink" title="[FIXME] [EP10] 多机DeepSeek优化部署"></a>[FIXME] [EP10] 多机DeepSeek优化部署</h1><p>直播回看链接：<a href="https://www.youtube.com/watch?v=UMf5-K4PX8Q">https://www.youtube.com/watch?v=UMf5-K4PX8Q</a></p><p>特别鸣谢：组织者@月球大叔, 主讲人@Du Kuntai, @Cheng Yihua</p><p>飞行嘉宾是Perplexity AI的Research Engineer、Punica的作者Chen Lequn</p><h2 id="💥-背景与介绍"><a href="#💥-背景与介绍" class="headerlink" title="💥 背景与介绍"></a>💥 背景与介绍</h2><p><strong>Lequn</strong>: 在大多数系统中，延迟和吞吐是相互冲突的目标。</p><ul><li><p>延迟——用户感知</p></li><li><p>吞吐——成本</p></li></ul><p>但Deepseek这种MOE模型在大多数场景下利用多机部署中更多的GPU可以达到更高的吞吐和更低的延迟。</p><h3 id="为什么要用DP？"><a href="#为什么要用DP？" class="headerlink" title="为什么要用DP？"></a>为什么要用DP？</h3><p>通过EP，我们可以将MoE计算分散在128个甚至更多的GPU上。如果单纯地将MLA按EP数进行TP，每个GPU分到的注意力头数量较少，会导致收益递减。这时，我们可以引入数据并行(Data Parallelism, DP)。每个DP组都有一个完整的MLA层副本。每个DP组接受不同的输入并独立执行MLA层计算。</p><p>MLA层的DP和TP可以组合，一个DP组可以被分成多个TP Rank。MoE层的EP可以与MLA层的DP&#x2F;TP组合。<code>EP = DP * TP</code>。例如，在16台机器上，EP128 DP32 TP4意味着将路由专家分布在128个GPU上，每4个GPU形成一个DP组，总共32个独立的DP组。</p><h4 id="有32个DP这么多GPU是怎么做到负载均衡的，或者对于高utilization问题，怎么做到负载均衡呢？"><a href="#有32个DP这么多GPU是怎么做到负载均衡的，或者对于高utilization问题，怎么做到负载均衡呢？" class="headerlink" title="有32个DP这么多GPU是怎么做到负载均衡的，或者对于高utilization问题，怎么做到负载均衡呢？"></a>有32个DP这么多GPU是怎么做到负载均衡的，或者对于高utilization问题，怎么做到负载均衡呢？</h4><p><img src="/posts_img/FIXME-EP10/1.png"></p><p><strong>Lequn</strong>: 这就是为什么我们前面会有一个scheduler，它相当于知道所有实例的负载情况，可以去做负载均衡。然后注意到其实这个scheduler也不需要跑的特别快，它不是像ingress那种外部服务器的那种负载均衡，因为qps不会太大，1000已经是一个比较大的体量了。</p><p><img src="/posts_img/FIXME-EP10/2.png"></p><p><strong>Yihua, Kuntai</strong>: LMCache团队开发的一个计算kv cache的小工具，可以在浏览器搜索“KV Cache Size Calculator”找到，但是现在计算Deepeek的kv cache结果是错误的（Deepseek-671B中一个token占用70272 bytes的KV缓存），欢迎去提pr!</p><h3 id="不同配置下的性能分析"><a href="#不同配置下的性能分析" class="headerlink" title="不同配置下的性能分析"></a>不同配置下的性能分析</h3><p><strong>Lequn</strong>: 使用一台H200机器进行单机部署，最多16台H100机器进行多机部署。对于每个部署环境，我们使用TP 1、2、4、8和每卡批处理大小1、2、4、8、16、32、64、128的组合。下图显示了不同配置的吞吐量和输出速度。</p><p><img src="/posts_img/FIXME-EP10/3.png"></p><p>横轴表示每个请求的输出速度(词元&#x2F;秒)。纵轴使用对数刻度显示每台机器的吞吐量(词元&#x2F;秒)。我们用不同颜色的线标记了每个EP配置的帕累托边界。</p><h3 id="关键Insight"><a href="#关键Insight" class="headerlink" title="关键Insight"></a><strong>关键Insight</strong></h3><ol><li><strong>单机EP8配置</strong>：批处理大小为1时输出速度极快（&gt;100词元&#x2F;秒），但吞吐量低；增大批处理会激活更多专家，加重内存带宽压力，显著降低输出速度。  </li><li><strong>高EP数优势</strong>（如EP128）：每个GPU负载的专家更少，内存带宽压力小，增大批处理时输出速度更稳定，吞吐量更高（比单机高约5倍）。  </li><li><strong>多机部署对比</strong>：  <ul><li>更高EP值（如EP128）同时提升吞吐量和输出速度。  </li><li>极大批处理时，单机吞吐可能略优于多机（因NVLink带宽更高或实现限制），但受内存容量限制，多机仍是高吞吐场景的更优选择。</li></ul></li></ol><h3 id="核心结论"><a href="#核心结论" class="headerlink" title="核心结论"></a><strong>核心结论</strong></h3><ul><li><strong>低延迟场景</strong>：单机小批处理（EP8）。  </li><li><strong>高吞吐场景</strong>：多机高EP配置（如EP128），平衡速度与吞吐。</li></ul><p>有趣的是，在更大的批处理大小(每卡64个请求)上，我们观察到一个新现象：单机部署吞吐量略高于多机部署。部分原因是节点内NVLink的带宽高于节点间InfiniBand。另一部分是由于我们实现的限制。我们将在后面更详细地分析这种现象。</p><h3 id="Q-A"><a href="#Q-A" class="headerlink" title="Q&amp;A"></a><strong>Q&amp;A</strong></h3><p><strong>Listener</strong>: 说了一种非常好的理解方式，多机的性能提高可以把它理解成等效的内存带宽提升。因为在decode的时候出来，是内存受限的，把expert放到更多的GPU上，相当于提高了整个cluster的内存带宽。</p><p><strong>Lequn</strong>: 核心是MOE在做计算的时候，expert要从显存读到register里面，如果要读的expert数大，即使你提高batch，对于提升throughput是不利的，而EP可以缓解这个问题，在读expert数小的时候，提高batch能有效提高throughput。</p><hr><p><strong>Listener</strong>: 对于不同的request，可能路由到不同的expert上，如何避免一些expert overload，一些expert underload？</p><p><strong>Lequn</strong>: 对于overload的expert做replica，调整冗余专家的放置就好了，EPLB。</p><p>其实这里Deepseek报告里有说。</p><hr><p>System N把斧：cache, batch, shard, replica, overlap…</p><p>今天介绍的内容主要是解决<strong>decode</strong>阶段的难点，不太考虑<strong>prefill</strong>。</p><p><strong>Character.AI</strong>，<strong>Mooncake</strong>也公布了自己对于推理优化的一些技术，感兴趣的可以看我上一篇文章，或搜索”Character AI inference blog”即可。</p><h2 id="📌-计算和通信重叠"><a href="#📌-计算和通信重叠" class="headerlink" title="📌 计算和通信重叠"></a>📌 计算和通信重叠</h2><p>如上文关于专家并行的介绍，GPU在MoE层通信期间是空闲的。为减少浪费并降低延迟，我们需要找到数据独立的计算任务来填充这个空闲时间。</p><p><img src="/posts_img/FIXME-EP10/4.png"></p><p>主要优化在于使用DeepSeek技术报告中提到的 micro batch 来打破数据依赖。如图下部所示，我们将一个Transformer层的计算分为5个阶段：</p><ul><li>阶段1：InputNorm, QKVProj, AppendKV, BMM</li><li>阶段2：BMM, Attn, OProj, PostNorm, Gate</li><li>阶段3：Dispatch Send, Shared Expert</li><li>阶段4：Dispatch Recv, MoE, Combine Send</li><li>阶段5：Combine Recv</li></ul><p>在前3个密集Transformer层中，我们使用整个 batch。在接下来的58个MoE Transformer层中，我们将 batch 平均分成两个 micro batch。两个 micro batch 交替执行，相差3个阶段。由于这两个 micro batch 之间没有数据依赖，我们可以在Dispatch Send和Combine Send之后切换到另一个 micro batch 的计算。</p><h3 id="Q-A-1"><a href="#Q-A-1" class="headerlink" title="Q&amp;A"></a><strong>Q&amp;A</strong></h3><p><strong>Yihua</strong>: 在不同micro batch之间切换如何实现？</p><p><strong>Lequn</strong>: 两个办法。1. 保存中间结果，相当于你手动维护这个状态机。2. 利用python语法糖，比如yield。</p><hr><p><strong>Yihua</strong>: 用python yield会影响cuda graph吗？</p><p><strong>Lequn</strong>: 不影响，会先走一个capture，这是一个静态的执行。</p><hr><p><strong>月球大叔</strong>：这个overlap是有两个cuda stream吗？</p><p><strong>Lequn</strong>: 好问题。只需要1个cuda stream就行了，两个micro batch在计算的时候是没有重叠的部分的，在算的时候搞对输入输出就行了。</p><p>Deepseek公开了一个trace，可以用Perfetto打开看kernel是怎么运行的</p><p><img src="/posts_img/FIXME-EP10/5.png"></p><p><img src="/posts_img/FIXME-EP10/6.png"></p><p>基于上面的trace，Lequn做了一个分析，包括函数、stage之间的latency占比</p><h4 id="有意思的是，使用micro-batch并不是在所有情况都是好的"><a href="#有意思的是，使用micro-batch并不是在所有情况都是好的" class="headerlink" title="有意思的是，使用micro batch并不是在所有情况都是好的"></a>有意思的是，使用micro batch并不是在所有情况都是好的</h4><p><img src="/posts_img/FIXME-EP10/7.png"></p><p>在高batch size的时候效果才比较好，可能的原因是在batch小的时候，再做拆分会使每个kernel计算效率降低。</p><h4 id="整个layer的latency"><a href="#整个layer的latency" class="headerlink" title="整个layer的latency"></a>整个layer的latency</h4><p><img src="/posts_img/FIXME-EP10/8.png"></p><h3 id="关键Insight-1"><a href="#关键Insight-1" class="headerlink" title="关键Insight"></a><strong>关键Insight</strong></h3><ol><li>比较EP8和EP128 Microbatch。EP8总共花费1802微秒，略少于EP128的1896微秒。除了上面提到的Microbatch带来的内核执行时间增加外，主要差异在于用于MoE计算的GroupGEMM，以及两个通信内核Dispatch和Combine。</li><li>EP8的GroupGEMM比EP128的时间减少了一半。这是多机部署的核心优势。</li><li>不幸的是，通信花费的时间增加了213微秒，这大大抵消了GroupGEMM的优势。在我们通信内核的单独性能测试中，我们发现它们只能达到Infiniband带宽的一半。我们将继续优化通信内核。</li><li>另一个明显拖后腿的内核是GEMM。Microbatch增加了GEMM 95微秒。</li></ol><h3 id="Roofline-GROUPGEMM"><a href="#Roofline-GROUPGEMM" class="headerlink" title="Roofline: GROUPGEMM"></a>Roofline: GROUPGEMM</h3><p>Lequn: 我们使用DeepGEMM的GroupGEMM实现进行性能测试。测试点覆盖了EP8、EP16、EP32、EP64、EP128配置与TP1和批处理大小1-128的组合。</p><p><img src="/posts_img/FIXME-EP10/9.png"></p><p>上图显示了不同EP配置下GroupGEMM的屋顶线模型。不同的EP对应不同数量的group。图中展示了几乎重叠的性能曲线，表明GroupGEMM性能主要由总词元数(表示为g * m)决定。</p><p>星号标记了每个EP配置下对应于每个GPU批处理大小为128的数据点。比较这些星号数据点，我们可以看到随着EP增加(DP也同步增加)，每个专家的词元数m也增加。在EP8时，m&#x3D;128，而在EP128时，m&#x3D;2048。</p><p>随着m增加，算术强度也增加。在大多数配置中，GroupGEMM受内存带宽限制，因此增加m可以提高性能。</p><p><strong>Kuntai</strong>: 分享一个talk, compute每年增速约1.3倍，但memory为1.2倍。未来会有更多的operater进入到memory bound中。</p><h4 id="更多的内容请自行搜索“多机部署DeepSeek实现更低延迟和更高吞吐量”的博文！"><a href="#更多的内容请自行搜索“多机部署DeepSeek实现更低延迟和更高吞吐量”的博文！" class="headerlink" title="更多的内容请自行搜索“多机部署DeepSeek实现更低延迟和更高吞吐量”的博文！"></a>更多的内容请自行搜索“多机部署DeepSeek实现更低延迟和更高吞吐量”的博文！</h4><h2 id="🔔-LMCache中为什么要用buffer？"><a href="#🔔-LMCache中为什么要用buffer？" class="headerlink" title="🔔 LMCache中为什么要用buffer？"></a>🔔 LMCache中为什么要用buffer？</h2><p>Yihua:</p><ol><li>page size太小的时候，可能打不满传输的带宽，一个可能考虑的解决方案是有一个稍微大一点的transfer buffer, 然后可以做device to device的，就是inter device的一些简单的copy。把东西都整合到一个大的buffer里的时候，再一起发出去。这样底层网络传输的带宽利用率会更高。但是它有一个extra copy的overhead，有一个trade off。</li><li>在使用CPU进行MemCopy的时候，像vllm这种page size小的，如果对每一个layer launch一个kernel，每一个page的16个token做一次，这个开销是比较大的。如果把这些pages都collect到一起，然后再做一次copy的PCIe transfer可能会更快一些。</li><li>还有一个办法是，直接把host memory swap到GPU的UBM上面…..（这里没听清楚，不懂）</li></ol><h2 id="⚡-Kuntai布置的几个小作业"><a href="#⚡-Kuntai布置的几个小作业" class="headerlink" title="⚡ Kuntai布置的几个小作业"></a>⚡ Kuntai布置的几个小作业</h2><ol><li><p>多模型部署，怎么在production stack里面部署多个不同类型的LLM，比如一个qwen-3B和一个LLama-8B模型。</p></li><li><p>如何开启Session-base router。根据请求的session-id做路由。</p></li><li><p>Mooncake kv cache hit rate是多少？</p></li></ol><p>前两个问题比较基础，用过production-stack的人应该都知道，我最近也在它上面做了一些测试和优化，感兴趣或有疑问的小伙伴欢迎来交流讨论！第三个问题，从mooncake仓库里可以看到相关描述“up to 50% cache hit ratio”。</p><h3 id="Ref"><a href="#Ref" class="headerlink" title="Ref:"></a>Ref:</h3><ol><li><a href="https://zhuanlan.zhihu.com/p/1896575231079993393">https://zhuanlan.zhihu.com/p/1896575231079993393</a></li><li><a href="https://zhuanlan.zhihu.com/p/1890996026371973366">https://zhuanlan.zhihu.com/p/1890996026371973366</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
            <tag> MoE </tag>
            
            <tag> DeepSeek </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EP07-MoE+闲谈学术品味</title>
      <link href="/2025/05/02/vLLM-EP07/"/>
      <url>/2025/05/02/vLLM-EP07/</url>
      
        <content type="html"><![CDATA[<h1 id="FIXME-EP07-聊聊MoE-闲谈学术品味"><a href="#FIXME-EP07-聊聊MoE-闲谈学术品味" class="headerlink" title="[FIXME] [EP07] 聊聊MoE+闲谈学术品味"></a>[FIXME] [EP07] 聊聊MoE+闲谈学术品味</h1><p>直播回看链接：<a href="https://www.youtube.com/watch?v=mHUBwzlsWjg">https://www.youtube.com/watch?v=mHUBwzlsWjg</a></p><p>特别鸣谢：组织者@月球大叔, 主讲人@Du Kuntai, @Cheng Yihua</p><p>飞行嘉宾是Google Deepmind高级研究科学家、OpenMoE的作者Xue Fuzhao </p><ul><li>主要方向：Gemini Pretraining、Model Architecture和Multi-Modal LLM</li><li>主要工作：OpenMoE, Token-Crisis, AdaTape, Sequence Parallelism和LongVILA.</li></ul><h2 id="💥-开场"><a href="#💥-开场" class="headerlink" title="💥 开场"></a>💥 开场</h2><p><strong>Fuzhao: 我的PhD七个收获</strong></p><ol><li><strong>工程能力</strong>是研究的基础。</li><li>与有才华的人合作对提升<strong>研究品味</strong>非常有帮助。</li><li>目标是<strong>一个简洁且有洞察力的45分钟演讲</strong>，而不是整个PhD期间的长篇论文列表。</li><li><strong>专注于少量关键论文</strong>并深入理解，而不是快速浏览大量论文。</li><li>当接触一个新主题时，<strong>沿着时间线阅读论文</strong>，以研究趋势的演变。<ul><li>“监督式练习”：基于当前论文，我接下来会做什么？</li></ul></li><li><strong>换位思考</strong>是提升写作和演讲的高效方法。</li><li><strong>PhD学位是有帮助的，但不是从事LLM研究的必要条件</strong>。</li></ol><h2 id="OpenMoE"><a href="#OpenMoE" class="headerlink" title="OpenMoE"></a>OpenMoE</h2><h3 id="The-original-MoE"><a href="#The-original-MoE" class="headerlink" title="The original MoE"></a>The original MoE</h3><p>常见误解：MoE是许多<em><strong>模型</strong></em>的混合。</p><p>实际上，它是一个单一模型，在FFN层中有多个<em><strong>专家</strong></em>。</p><p><strong>Router</strong>决定激活哪些专家。</p><p>Router是一个简单的<strong>线性层</strong>，通过点积运算。</p><p>图示来自<em>Switch Transformers (2022)</em></p><p><a href="https://arxiv.org/abs/2101.03961">https://arxiv.org/abs/2101.03961</a></p><p><img src="/posts_img/vLLM-EP07/image.png"></p><h3 id="History-of-MoEs"><a href="#History-of-MoEs" class="headerlink" title="History of MoEs"></a>History of MoEs</h3><h4 id="2017-LSTM-MoE"><a href="#2017-LSTM-MoE" class="headerlink" title="2017: LSTM + MoE"></a>2017: LSTM + MoE</h4><p>Gating network</p><ul><li>Naive hard selection不可微分。</li><li>这项工作使用了一些技巧使其可微分。</li></ul><p><img src="/posts_img/vLLM-EP07/image-1.png"></p><h4 id="2021-Gshard-from-Google-MLSys"><a href="#2021-Gshard-from-Google-MLSys" class="headerlink" title="2021: Gshard from Google (MLSys)"></a>2021: Gshard from Google (MLSys)</h4><p>MoE：增加参数数量而不增加FLOPs → 更稀疏 → 更节省内存。</p><p>Expert parallelism：不同的专家分布在不同的GPU上。</p><p>引入了额外的all-to-all通信开销。</p><p><img src="/posts_img/vLLM-EP07/image-2.png"></p><h4 id="2021-Switch-Transformer"><a href="#2021-Switch-Transformer" class="headerlink" title="2021: Switch Transformer"></a>2021: Switch Transformer</h4><p>第一个开源的MoE，非常重要的工作。</p><p>基于T5构建。由于T5太大，使用它的人不多。</p><p><img src="/posts_img/vLLM-EP07/image-3.png"></p><p>后续工作：GLaM, ST-MoE…</p><p>Awesome-mix-of-experts: 10篇必读论文 <a href="https://github.com/XueFuzhao/awesome-mixture-of-experts">https://github.com/XueFuzhao/awesome-mixture-of-experts</a></p><p>Google当时做了很多相关工作。</p><h4 id="2023-OpenMoE-→-一系列开源MoE模型"><a href="#2023-OpenMoE-→-一系列开源MoE模型" class="headerlink" title="2023: OpenMoE → 一系列开源MoE模型"></a>2023: OpenMoE → 一系列开源MoE模型</h4><p>GPT4使用了MoE</p><p><strong>Fuzhao当时为什么选择MoE</strong>：在扩展规模时，MoE依然表现良好</p><p>在TPU上训练了一年。</p><p>现在性能一般。主要贡献是<strong>可视化带来的insights</strong>。</p><p>Kuntai：MoE会越来越流行，因为它性价比高。vLLM正在尝试更高效地支持MoE</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul><li>2017: LSTM + MoE</li><li>2021: Gshard (MoE MLSys)</li><li>2021: Switch Transformer (MoE)</li><li>GLaM, ST-MoE</li><li>2023: OpenMoE, Mixtral</li><li>2024: DeepSeek-MoE, OLMoE <a href="https://github.com/allenai/OLMoE?tab=readme-ov-file">https://github.com/allenai/OLMoE?tab=readme-ov-file</a></li><li>2025: LLaMA-4</li></ul><p>MoE曾经很难训练</p><h2 id="📌-Formal-definiton"><a href="#📌-Formal-definiton" class="headerlink" title="📌 Formal definiton"></a>📌 Formal definiton</h2><p>给定E个专家：</p><p>$$<br>MoE(x) &#x3D; \sum_{i&#x3D;1}^{E} g(x)_i e_i(x)<br>$$</p><p>其中<code>e_i()</code>是第i个专家的非线性变换，<code>g()_i</code>是可训练的router <code>g()</code>输出的第i个元素。</p><p>通常，<code>e()</code>和<code>g()</code>都由神经网络参数化。</p><p>g()扩展实验：没有明显收益</p><h3 id="TopK-selection"><a href="#TopK-selection" class="headerlink" title="TopK selection"></a>TopK selection</h3><p>为了保证<code>g()</code>的稀疏路由，使用<code>TopK()</code>选择排名靠前的专家。</p><p>公式如下：</p><p>$$<br> \text{g(x) &#x3D;TopK(softmax(f(x) + ε))}<br>$$</p><p>其中<code>f()</code>是路由线性层，<code>ε</code>是用于探索专家路由的高斯噪声。</p><h3 id="Balanced-loading"><a href="#Balanced-loading" class="headerlink" title="Balanced loading"></a>Balanced loading</h3><p>EP：如果4个专家中有一个很热门，3个GPU经常空闲 → 3个E的权重被浪费</p><p>不均衡的token分配会降低MoE模型的吞吐量和性能。</p><p>为防止这种情况，需要避免两个问题：</p><ol><li><strong>太多token</strong>被<strong>发送</strong>到某一个专家</li><li><strong>太少token</strong>被<strong>分配</strong>给某一个专家</li></ol><h4 id="1-太多token被发送"><a href="#1-太多token被发送" class="headerlink" title="1. 太多token被发送"></a><strong>1. 太多token被发送</strong></h4><p>为了解决第一个问题，定义了<strong>buffer capacity B</strong>：B &#x3D; CKNL</p><p>其中：</p><ul><li>C: capacity ratio</li><li>K: 每个token选择的专家数</li><li>N: 每个设备上的batch size</li><li>L: 序列长度</li></ul><p>对于每个专家，无论分配多少token，<strong>最多只保留B个token</strong>。</p><h4 id="2-太少token被分配"><a href="#2-太少token被分配" class="headerlink" title="2. 太少token被分配"></a>2. <strong>太少token被分配</strong></h4><p>为了解决第二个问题，在训练时在总损失中加入了如下<strong>辅助损失</strong>：</p><p>$$<br>l_{balance} &#x3D; E \cdot \sum_{i&#x3D;1}^{E} m_i \cdot P_i <br>$$</p><ul><li><code>m_i</code>：分配给专家<code>i</code>的token比例</li><li><code>P_i</code>：router内部第i个专家的softmax输出（即概率值）。</li></ul><p>$$<br>m_i &#x3D; \frac{1}{L} \sum_{j&#x3D;1}^{L} h(x_j)_i <br>$$</p><ul><li><code>L</code>：token总数（batch size × 序列长度）。</li><li><code>h(x_j)_i</code>：对于第j个token，表示是否分配给专家（是为1，否则为0）。</li></ul><p>所以<code>m_i</code>是：</p><ul><li>统计有多少token被路由到专家iii，</li><li>然后除以token总数，得到<strong>专家iii的负载比例</strong>。</li></ul><p><code>m</code>是不可微的。因此，定义可微的<code>P_i</code>：</p><p>$$<br>P_i &#x3D; \text{softmax}(f(x) + \epsilon)_i <br>$$</p><p>当我们最小化<code>l_&#123;balance&#125;</code>时，可以看到<code>m</code>和<code>P</code>都会趋于均匀分布。</p><p>注意：<code>P_i</code>是router中Top K的<strong>输入</strong></p><p>$$<br>g(x) &#x3D; \text{TopK}(\text{softmax}(f(x) + \epsilon)) <br>$$</p><p>当时，MoE可以节省50% FLOPs。现在可能会更好。</p><h3 id="Training-on-MoE-multi-turn-is-worse"><a href="#Training-on-MoE-multi-turn-is-worse" class="headerlink" title="Training on MoE: multi-turn is worse?"></a>Training on MoE: multi-turn is worse?</h3><p>MTBench：早期的多轮对话评测基准。</p><p>多轮结果比单轮差。类似地，few shot比zero shot差。原因后面会讨论。</p><p><img src="/posts_img/vLLM-EP07/477dc85a-efe2-4d24-8b70-814690bf575c.png"></p><h3 id="MoE会专门化吗？"><a href="#MoE会专门化吗？" class="headerlink" title="MoE会专门化吗？"></a>MoE会专门化吗？</h3><p>领域层面？没有专门化。token被均匀分配到不同E。</p><p><img src="/posts_img/vLLM-EP07/7675cc3f-05d4-4b90-a639-c15eeaf47fd3.png"></p><p>编程语言？没有</p><p>自然语言？有一定程度。相似语言会分到相似E。</p><p><img src="/posts_img/vLLM-EP07/d9ed9a79-1559-45f4-90b0-dc4e37d02785.png"></p><p><img src="/posts_img/vLLM-EP07/738e10de-667a-4dfd-9aa7-2c19efb355bb.png"></p><p>Yihua：为什么有些E擅长很多任务。</p><p>Fuzhao：一种可能是E处理system prompt。</p><p>MoE只是懒得按位置分配吗？没有明显规律。</p><p><img src="/posts_img/vLLM-EP07/a93c893d-f2ce-4f0f-8bdc-1e7248ba291f.png"></p><h3 id="Context-independent-specialization"><a href="#Context-independent-specialization" class="headerlink" title="Context-independent specialization"></a><strong>Context-independent specialization</strong></h3><p>Token ID专门化？<strong>是的</strong></p><p><img src="/posts_img/vLLM-EP07/7458f9f0-0d4d-42bc-9cb1-3d5caf45cb1f.png"></p><p>Router只是线性层。没有学到高层语义，只是按token ID路由。</p><p>Deepseek V2也是这样。Deepseek V3还没测。</p><p>观众：Deepseek R1没有token专门化</p><h4 id="专家会聚类相似token吗？"><a href="#专家会聚类相似token吗？" class="headerlink" title="专家会聚类相似token吗？"></a>专家会聚类相似token吗？</h4><p>会</p><p><img src="/posts_img/vLLM-EP07/3e85675f-7e39-4485-8dbc-526cdb514663.png"></p><p>E21喜欢编程token</p><p>E31喜欢情态动词</p><p>E0和E1喜欢’\n’</p><h3 id="为什么Context-independent？早期路由学习"><a href="#为什么Context-independent？早期路由学习" class="headerlink" title="为什么Context-independent？早期路由学习"></a>为什么<strong>Context-independent？早期路由学习</strong></h3><p><img src="/posts_img/vLLM-EP07/0c5ab82f-7c10-485e-9321-753b14b4bc1d.png"></p><p>路由决策在高层语义还没学好时就已经学会了。</p><p><strong>QA: MoE在消费级GPU上</strong></p><ul><li>E经常在CPU内存&#x2F;磁盘中换进换出</li><li>如何减少I&#x2F;O？</li><li>基于相关性预取E。例如，如果Ex在第i层被激活，那么Ey很可能在第i+1层被激活。</li></ul><p>任务专门化其实就是token专门化：</p><ul><li>编程专家：喜欢编程token</li><li>数学专家：喜欢数字token</li><li>…</li></ul><p><a href="https://arxiv.org/abs/2412.14219">https://arxiv.org/abs/2412.14219</a></p><h3 id="末端token丢失"><a href="#末端token丢失" class="headerlink" title="末端token丢失"></a>末端token丢失</h3><p><img src="/posts_img/vLLM-EP07/image-4.png"></p><p>为什么多轮更差？为什么few shot更差？看起来context越长越差。</p><p>回忆一下capacity ratio：如果E收到太多token，会丢弃<strong>后面的</strong>token。</p><p>即使有负载均衡，分配到某个E的token还是可能太多。</p><ul><li>推理分布和训练分布可能差异很大。例如，短时间内可能有大量编程任务。</li></ul><p>context越长，负载越不均衡。</p><p>SFT阶段能解决吗？尝试过但失败了。</p><p>Kuntai：P&#x2F;D解耦能解决吗？</p><p>Yihua：chunked prefill能解决吗？</p><p>Fuzhao：最近Megablock工作没有C</p><h3 id="近期进展：Megablock"><a href="#近期进展：Megablock" class="headerlink" title="近期进展：Megablock"></a>近期进展：Megablock</h3><p>最近的MoE去掉了capacity ratio C → dropless-MoE (dMoE) → 负载不均衡</p><ul><li>→ 降低EP。例如每个GPU 8或16个专家</li><li>→ MegaBlocks：将稠密批量matmul转为稀疏matmul</li></ul><p><a href="https://github.com/databricks/megablocks">https://github.com/databricks/megablocks</a></p><p><a href="https://arxiv.org/abs/2211.15841">https://arxiv.org/abs/2211.15841</a></p><p><img src="/posts_img/vLLM-EP07/image-5.png"></p><p><img src="/posts_img/vLLM-EP07/image-6.png"></p><p><img src="/posts_img/vLLM-EP07/image-7.png"></p><blockquote><p>图3. <strong>MoE层中的专家计算</strong>。以num expert&#x3D;3为例。</p><p>(A) 目前最先进的MoE实现使用批量矩阵乘法并行计算所有专家。这要求所有专家分配相同数量的token且形状一致。</p><p>(B) 专家计算也可以用块对角矩阵乘法（块大小相同）来表示。</p><p>(C) 为了放宽这些约束，可以构造由许多小块组成的变尺寸块对角矩阵。<strong>可以用block-sparse matrix multiplication高效计算。</strong></p></blockquote><p>有很多方法可以高效实现<strong>block-sparse matmul</strong>。</p><p>新趋势：attention-MoE解耦</p><h3 id="DeepSeek-MoE"><a href="#DeepSeek-MoE" class="headerlink" title="DeepSeek MoE"></a>DeepSeek MoE</h3><ul><li>更多E，每个E更小：路由更平滑</li><li>激活更多E</li><li>Shared E：始终开启<ul><li>有EP时，至少有一个GPU不会空闲（可以做shared E计算）</li></ul></li></ul><p><img src="/posts_img/vLLM-EP07/7bd7ae81-9c9e-4609-84ab-c2eb737d5eb3.png"></p><p><strong>KTransformers</strong>：attention在GPU上，MoE在CPU上</p><p><a href="https://kvcache-ai.github.io/ktransformers/en/deepseek-v2-injection.html">https://kvcache-ai.github.io/ktransformers/en/deepseek-v2-injection.html</a></p><p><a href="https://github.com/kvcache-ai/ktransformers">https://github.com/kvcache-ai/ktransformers</a></p><h3 id="更多思考"><a href="#更多思考" class="headerlink" title="更多思考"></a>更多思考</h3><ul><li>MoE训练很棘手（负载均衡、训练稳定性）<ul><li>例子：UL2在OpenMoE 34B参数时变得不稳定，只能用50B token训练OpenMoE 34B</li></ul></li><li>MoE更需要大量数据<ul><li>MoE模型在有限或重复数据上容易过拟合</li></ul></li><li>MoE对数据多样性更敏感</li></ul><p><strong>DeepSeek</strong></p><ul><li>专家级无损路由</li><li>设备级：每个GPU 4个E，GPU间负载均衡，同一GPU内E可以不均衡</li></ul><p><img src="/posts_img/vLLM-EP07/image-8.png"></p><p><img src="/posts_img/vLLM-EP07/215f11cb-ea62-404b-807f-1077fcc2accc.png"></p><p><strong>Dense upscaling</strong>：稠密模型已学到语义，路由更均衡</p><p><strong>Token-choice vs expert-choice MoE</strong></p><p><img src="/posts_img/vLLM-EP07/image-9.png"></p><ul><li>Token-choice：每个token选择E</li><li>Expert-choice：所有token进来，每个E选择要接受哪些token<ul><li>存在context泄漏问题。不能用于decoder<ul><li>除非batch size非常大，只在batch维度选择</li></ul></li></ul></li></ul><h3 id="当时的Takeaways"><a href="#当时的Takeaways" class="headerlink" title="当时的Takeaways"></a>当时的Takeaways</h3><ul><li><strong>优点</strong><ul><li>MoE能节省约50%训练成本（现在甚至更多）</li><li>MoE推理成本节省不到50%</li></ul></li><li><strong>缺点</strong><ul><li>MoE训练棘手（负载均衡、训练稳定性）</li><li>MoE更需要大量数据</li><li>MoE对数据多样性更敏感</li><li>MoE通信开销大（EP，不太友好硬件）</li></ul></li></ul><p>什么时候用MoE？</p><ul><li>知识很重要 → 用更多参数存储知识</li><li>有大量并发请求 → 可以高效利用更多专家</li><li>有足够数据时</li><li>有更多时间debug时（至少目前阶段）😊</li></ul><h2 id="🔔-vLLM-MoE支持"><a href="#🔔-vLLM-MoE支持" class="headerlink" title="🔔 vLLM MoE支持"></a>🔔 vLLM MoE支持</h2><p>核心：处理EP</p><p>Route → MoE → Route back</p><p>(这次主要是闲聊，vllm源码就不讲啦~)</p><h2 id="⚡-Kuntai-什么是好-坏研究"><a href="#⚡-Kuntai-什么是好-坏研究" class="headerlink" title="⚡ Kuntai: 什么是好&#x2F;坏研究"></a>⚡ Kuntai: 什么是好&#x2F;坏研究</h2><h4 id="Insight-1-人们对什么是糟糕的研究有广泛共识，但对什么是好的研究没有共识。"><a href="#Insight-1-人们对什么是糟糕的研究有广泛共识，但对什么是好的研究没有共识。" class="headerlink" title="Insight 1: 人们对什么是糟糕的研究有广泛共识，但对什么是好的研究没有共识。"></a><strong>Insight 1:</strong> 人们对什么是糟糕的研究有广泛共识，但对什么是好的研究没有共识。</h4><ul><li>一篇USENIX Security论文的研究结果：<ul><li>科学的科学：科学研究背后的规则或趋势。</li></ul></li><li>糟糕研究的例子（主要是技术问题）：<ul><li>不一致。</li><li>不遵循逻辑 → 含糊其辞。</li><li>图表不对齐 → 表明不一致。</li><li>没有动机。</li><li>没有说明为什么它有价值（这不仅仅是研究中的普遍收获）。</li></ul></li><li>PhD培训的目标 → 你知道什么是糟糕的研究，并知道如何避免它。</li></ul><hr><p><strong>但人们不知道什么是好的研究</strong>（远超技术问题）。</p><ul><li>为什么：好的研究与人们的价值体系紧密相关。<ul><li>研究者A：我喜欢有扎实测量的论文。（例如，工业研究者）</li><li>研究者B：我喜欢有清晰抽象的论文。（例如，欧洲系统社区）</li><li>研究者C：我喜欢有硬件数据的论文。（例如，移动社区）</li><li>研究者D：我喜欢实现SoTA的论文。（例如，ML社区）</li></ul></li><li>即使人们对好的研究有价值体系，在短期内做好的研究并不一定有回报。</li></ul><hr><h4 id="Insight-2-区分最优秀的工程师-最优秀的人不是他们的技能-能力，而是他们的选择。"><a href="#Insight-2-区分最优秀的工程师-最优秀的人不是他们的技能-能力，而是他们的选择。" class="headerlink" title="Insight 2: 区分最优秀的工程师&#x2F;最优秀的人不是他们的技能&#x2F;能力，而是他们的选择。"></a><strong>Insight 2:</strong> 区分最优秀的工程师&#x2F;最优秀的人不是他们的技能&#x2F;能力，而是他们的选择。</h4><ul><li>关键：尝试做出一个好的选择，然后全力以赴。</li><li>如何做出好的选择：沟通是关键。广泛与他人沟通，然后做出自己的选择。</li><li>（个人观点）趋势&#x2F;未来非常难以预测，充满巧合 → 我们最好领先他人半步 → 做开源研究，贴近工业。</li></ul><hr><p><strong>系统社区：</strong> 应用非常流行 → 但我们在实践中难以扩展它们 → 我们需要更好的系统。</p><ul><li>关键词：非常务实，在大规模上评估，与流行应用紧密结合。</li></ul><p><strong>为什么一些教授不希望学生进入工业界：</strong></p><ul><li>因为研究的风险远小于在现实世界中做事情。</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
            <tag> vLLM </tag>
            
            <tag> MoE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EP06-vLLM源码讲解直播笔记-vLLM v1 仙人指路</title>
      <link href="/2025/04/21/vLLM-EP06/"/>
      <url>/2025/04/21/vLLM-EP06/</url>
      
        <content type="html"><![CDATA[<h1 id="FIXME-EP06-vLLM-源码讲解直播笔记"><a href="#FIXME-EP06-vLLM-源码讲解直播笔记" class="headerlink" title="[FIXME][EP06] vLLM 源码讲解直播笔记"></a>[FIXME][EP06] vLLM 源码讲解直播笔记</h1><h2 id="EP06-vLLM-v1-仙人指路"><a href="#EP06-vLLM-v1-仙人指路" class="headerlink" title="EP06: vLLM v1 仙人指路"></a>EP06: vLLM v1 仙人指路</h2><p>直播回看链接：<a href="https://www.youtube.com/watch?v=6AcgEPmpHIc">https://www.youtube.com/watch?v=6AcgEPmpHIc</a></p><p>特别鸣谢：组织者@月球大叔, 主讲人@Du Kuntai, 飞行嘉宾@YM</p><p>vLLM官方博客：<a href="https://blog.vllm.ai/2025/01/27/v1-alpha-release.html">https://blog.vllm.ai/2025/01/27/v1-alpha-release.html</a></p><h3 id="💥-1-Why-v1-Motivation"><a href="#💥-1-Why-v1-Motivation" class="headerlink" title="💥 1. Why v1 ? (Motivation)"></a>💥 1. Why v1 ? (Motivation)</h3><ul><li><p>vLLM v0 运行起来有点慢 (CPU overhead)</p></li><li><p>vLLM v0 的代码可读性和可二次开发的能力较差</p><ul><li>比如v0的Scheduler代码有2k行，改进后的v1代码只有800行</li><li>代码改动牵一发而动全身</li></ul></li><li><p>如何推进代码重构?</p><ul><li>YM: 在稳定之后完成切换</li><li>开发完成后切换的问题：不切实际，新功能 &amp; 新模型的不断涌现</li><li>vLLM 最重要的特性: 对新模型的支持!!!<ul><li>易用</li><li>性能</li><li>day0 support(在新模型刚发布就适配) –&gt; tech debt(技术债，<br>  由于着急为新功能提供支持而导致工程上的不优雅，可能会影响后续新功能的支持)</li></ul></li></ul></li><li><p>vLLM重构代码的几个阶段</p><ul><li>Stage 1: v1 的开发</li><li>Stage 2: v0 &amp; v1 共存</li><li>Stage 3: 默认开启 v1 (现在)</li><li>Stage 4: v1 比 v0 具有更多可支持的功能</li><li>Stage 5: 移除 v0 的代码</li></ul></li><li><p>为什么Pytorch赢了Tensorflow? 这也是vLLM需要重构的原因</p><ul><li>Tensorflow曾经说过：我们有更多的功能，更好的性能和更多的硬件支持</li><li>原因：研究员更喜欢pytorch，然后他们毕业了…</li><li>vLLM v0 对研究员们不够友好</li></ul></li></ul><h3 id="📌-2-Scheduler"><a href="#📌-2-Scheduler" class="headerlink" title="📌 2. Scheduler"></a>📌 2. Scheduler</h3><p>代码：<code>vllm/v1/core/sched/scheduler.py</code></p><ul><li>调度：统一不同方法下调度tokens的逻辑<ul><li>比如一个长度为500 tokens的请求<ul><li>Prefill: {r: 500}</li><li>Decode: {r: 1}</li><li>Chunk prefill: 256, {r: 256}, {r: 244}</li><li>Prefix caching: r 命中了200个前缀token: {r: 300}</li></ul></li><li>Speculative decoding: 每个请求5个token {r: 5}</li><li>Multi-modality: r: 100个文本tokens, 500个图像tokens, 100个文本tokens:<ul><li>{r: 100}, {r: 500}, {r: 100}</li></ul></li></ul></li><li>简化调度逻辑，默认使用chunk prefill，不严格区分prefill和decode</li><li>同步调度</li></ul><h3 id="🔔-3-General-architecture"><a href="#🔔-3-General-architecture" class="headerlink" title="🔔 3. General architecture"></a>🔔 3. General architecture</h3><ul><li>前后端解耦，Scheduler, API Server, (de) tokenizer分在不同的进程上<br>  <img src="/posts_img/vLLM-EP06/v1_server_architecture.png?60">  <!-- <img src="posts_img/vLLM-EP06/v1_server_architecture.png" alt="v1 server architecture" width="600"> --></li><li>Scheduler &amp; Worker 在不同的进程中<ul><li>Scheduler, Rank 0 worker在同一个process中并存（在之前的v0版本上）<br>  <img src="/posts_img/vLLM-EP06/v1_tp_architecture.png?50">  <!-- <img src="posts_img/vLLM-EP06/v1_tp_architecture.png" alt="v1 tp architecture" width="500"> --></li></ul></li></ul><p>要获取最新的vLLM更新的干货，可以查看github仓库里meetup的slices</p><ul><li>例如：<a href="https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing">https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing</a></li></ul><h3 id="⚡-3-Worker"><a href="#⚡-3-Worker" class="headerlink" title="⚡ 3. Worker"></a>⚡ 3. Worker</h3><ul><li>Persistent batching<ul><li>对于从CPU到GPU之间的数据传输，我们只需要传上一个batch的tensor增量即可</li><li>这个技术不新</li><li>相关代码位于: <code>vllm/v1/worker/gpu_input_batch.py</code>, <code>vllm/v1/worker/gpu_worker.py</code></li></ul></li><li>Piecewise cudagraph<ul><li>Cudagraph<ul><li>记录了一系列CUDA kernel operation然后在之后重放</li><li>CPU 启动 CUDA kernel 是很慢的, 但是运行一个CUDA kernel是非常快的</li><li>CUDAGraph: 对于一系列的CUDA kernel, CPU只需要启动一次<ul><li>不会记录CPU operation, 丧失了灵活性</li></ul></li></ul></li><li>CUDAGraph的缺点： 丧失了灵活性</li><li>Observation: 灵活性需求通常发生在attention layer而不在MLP layer</li><li>解决方法：piece-wise cudagraph, 只在MLP层记录cuda graph，attention的部分使用pytorch eager mode</li></ul></li></ul><h3 id="💡-4-Attention-kernel"><a href="#💡-4-Attention-kernel" class="headerlink" title="💡 4. Attention kernel"></a>💡 4. Attention kernel</h3><ul><li>简化了设置<ul><li>Key observation: 对于每个attention kernel, 最基本的信息来源于大概6-7个tensors</li></ul></li><li>Cascade inference (级联推理)<ul><li>假设有这样的一个场景<ul><li>System prompt: 10,000 tokens</li><li>10 user chat, each chat 100 tokens</li></ul></li><li>常规的attention需要读取的内存大小<ul><li>（10，000 + 100） * 10 tokens</li></ul></li><li>Cascade inference<ul><li>10,000 + 100 * 10 tokens</li></ul></li><li>vLLM: 使用performance model去决定什么时候使用cascade inference<ul><li><code>vllm/v1/attention/backends/flash_attn.py: use_cascade attention</code></li></ul></li></ul></li></ul><h3 id="🔎-6-Multi-model"><a href="#🔎-6-Multi-model" class="headerlink" title="🔎 6. Multi-model"></a>🔎 6. Multi-model</h3><ul><li>Embedding as the KV cache reference</li><li>KV cache 管理（incoming）<ul><li>Hybrid memory allocator…</li></ul></li><li>这部分内容太多，值得开个专题来讲讲，下次见！</li></ul>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
            <tag> vLLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EP05-vLLM源码讲解直播笔记-Prefix Caching</title>
      <link href="/2025/04/16/vLLM-EP05/"/>
      <url>/2025/04/16/vLLM-EP05/</url>
      
        <content type="html"><![CDATA[<h1 id="FIXME-EP05-vLLM-源码讲解直播笔记"><a href="#FIXME-EP05-vLLM-源码讲解直播笔记" class="headerlink" title="[FIXME][EP05] vLLM 源码讲解直播笔记"></a>[FIXME][EP05] vLLM 源码讲解直播笔记</h1><h2 id="EP05-Prefix-Caching"><a href="#EP05-Prefix-Caching" class="headerlink" title="EP05: Prefix Caching"></a>EP05: Prefix Caching</h2><p>直播回看链接：<a href="https://www.youtube.com/watch?v=mWvqA_BNtsU">https://www.youtube.com/watch?v=mWvqA_BNtsU</a></p><p>特别鸣谢：月球大叔, Cheng Yihua, Kaz 大佬带来的精彩讲解</p><h3 id="📌-1-LLM使用KVCache进行推理的基本实现"><a href="#📌-1-LLM使用KVCache进行推理的基本实现" class="headerlink" title="📌 1. LLM使用KVCache进行推理的基本实现"></a>📌 1. LLM使用KVCache进行推理的基本实现</h3><p>输入: tokens, 已有的kvcache</p><p>输出: tokens, 更新后的kvcache</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">llm.interence(</span><br><span class="line">    input_tokens: <span class="built_in">list</span>[<span class="built_in">int</span>],  <span class="comment"># N tokens</span></span><br><span class="line">    previous_kv_cache: <span class="built_in">list</span>[Tensor],  <span class="comment"># N tokens&#x27; kv cache M &lt; N</span></span><br><span class="line">) -&gt; output_tokens, new_kv_cache</span><br><span class="line"></span><br><span class="line">output_tokens: <span class="comment"># N&#x27; new token</span></span><br><span class="line">new_kv_cache: <span class="comment"># kv cache of N + N&#x27; tokens</span></span><br></pre></td></tr></table></figure><p>抛开vllm还是sglang等主流框架的具体实现细节不谈，大家可以思考下，KVCache的管理有哪些特点，应该怎么去实现它的具体功能？</p><h3 id="⚡-2-Prefix-Matching-前缀匹配"><a href="#⚡-2-Prefix-Matching-前缀匹配" class="headerlink" title="⚡ 2. Prefix Matching (前缀匹配)"></a>⚡ 2. Prefix Matching (前缀匹配)</h3><p>KVCache的存取本质上与传统的键-值对数据库是类似的，如Redis等，Key: [tokens]，Value: [KV cache tensors]。</p><p>基本的功能包括KV的存储与获取，可以抽象成以下两个方法：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">KVCacheStore</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">store</span>(<span class="params">tokens, kv_cache_tensors</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">retrieve</span>(<span class="params">tokens</span>) -&gt; kkv_cache_tensors</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>与普通的键-值对数据库功能不同，KVCache具有前缀匹配的特点：</p><ul><li><p>假设我们有Tokens 1和Tokens 2: </p><p>Tokens 1: ABCD<code>E</code> -&gt; [KV1, KV2, KV3, KV4, <code>KV5</code>]</p><p>Tokens 2: ABCD<code>F</code> -&gt; [KV1, KV2, KV3, KV4, <code>KV6</code>]</p></li><li><p>存入Tokens 1的KV:</p><p>  kv_cache_store.store(“ABCDE”, [KV1, KV2, KV3, KV4, KV5])</p></li><li><p>取出Tokens 2的KV: </p><p>  kv_cache_store.retrieve(“ABCDF”) -&gt; [KV1, KV2, KV3, KV4]</p><p>  由于最后一个token不同，因此只能返回前缀相同部分的对应的KV</p></li><li><p><strong>Trie</strong>，也称为前缀树或字典树，是一种高效的树形数据结构，常用于存储和检索字符串集合。<br>可以让Trie的每个节点代表一个token，从根节点到某个节点的路径表示一个token序列，特别适合处理前缀匹配问题。</p></li></ul><h3 id="💡-3-vLLM中KVCache的设计"><a href="#💡-3-vLLM中KVCache的设计" class="headerlink" title="💡 3. vLLM中KVCache的设计"></a>💡 3. vLLM中KVCache的设计</h3><ul><li><p>首先，会将tokens根据chunk_size进行分块，假设chunk_size&#x3D;2</p><p>  “ABCDEF” -&gt; “AB”, “CD”, “EF” -&gt; list of chunked prefix hashes</p><p>  chunk_size的大小是一个trade-off，size太大，匹配的精确度低，size太小，I&#x2F;O的效率差。<br>  在vLLM中，chunk_size默认为16, sglang可以为1。</p><p>  （这里说的chunk_size对应vllm代码中的block_size，注意区分与chunk prefill中chunk_size的概念。）</p></li><li><p>然后，对chunks进行哈希，这里要注意的是，每个chunk的哈希值都包含了该chunk前缀的信息，<br>用于模拟前缀树的实现  </p>  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prefix_hash = <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> chunk_tokens:  <span class="comment"># [&quot;AB&quot;, &quot;CD&quot;, &quot;EF&quot;]</span></span><br><span class="line">    chunk_hash = <span class="built_in">hash</span>(prefix + chunk)</span><br><span class="line">    prefix_hash = chunk_hash</span><br></pre></td></tr></table></figure></li><li><p>接着是存取的基本实现</p>  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Given chunked prefix hashes, chunk kv cache</span></span><br><span class="line"><span class="comment"># store</span></span><br><span class="line"><span class="keyword">for</span> chunk_hash, chunk_kv <span class="keyword">in</span> <span class="built_in">zip</span>(...):</span><br><span class="line">    redis.put(chunk_hash, chunk_kv)</span><br><span class="line"></span><br><span class="line"><span class="comment"># retrieve</span></span><br><span class="line"><span class="keyword">for</span> chunk_hash <span class="keyword">in</span> ...:</span><br><span class="line">    kv_chunk = redis.get(chunk_hash)</span><br><span class="line">    <span class="keyword">if</span> kv_chunk <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure></li><li><p>Eviction（驱逐）</p><p>  随着推理的不断进行，当内存不够用了，需要对某些KV进行驱逐。</p><p>  Eviction的规则：对于request从后往前驱逐，尽可能复用prefix cache</p><ul><li>“ABCDEF” –&gt; [“AB”, KV1], [“CD”, KV2], [“EF”, KV3]</li><li>从KV3，KV2，KV1的顺序从后往前驱逐</li></ul><p>  可以使用LRU, LFU等驱逐策略</p></li></ul><h3 id="🔎-4-vLLM中KVCache的具体实现"><a href="#🔎-4-vLLM中KVCache的具体实现" class="headerlink" title="🔎 4. vLLM中KVCache的具体实现"></a>🔎 4. vLLM中KVCache的具体实现</h3><h4 id="Retrevie"><a href="#Retrevie" class="headerlink" title="Retrevie"></a>Retrevie</h4><p>“vllm&#x2F;v1&#x2F;core&#x2F;sched&#x2F;scheduler.py”</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Get already-cached tokens.</span></span><br><span class="line">computed_blocks, num_computed_tokens = \</span><br><span class="line">    <span class="variable language_">self</span>.kv_cache_manager.get_computed_blocks(request)</span><br></pre></td></tr></table></figure><p>get_computed_blocks的函数定义</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_computed_blocks</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self, request: Request</span>) -&gt; <span class="built_in">tuple</span>[<span class="built_in">list</span>[KVCacheBlock], <span class="built_in">int</span>]:</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># The block hashes for the request may already be computed</span></span><br><span class="line">        <span class="comment"># if the scheduler has tried to schedule the request before.</span></span><br><span class="line">        block_hashes = <span class="variable language_">self</span>.req_to_block_hashes[request.request_id]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> block_hashes:</span><br><span class="line">            <span class="comment"># 计算每个block的hash</span></span><br><span class="line">            block_hashes = hash_request_tokens(<span class="variable language_">self</span>.caching_hash_fn,</span><br><span class="line">                                               <span class="variable language_">self</span>.block_size, request)</span><br><span class="line">            <span class="variable language_">self</span>.req_to_block_hashes[request.request_id] = block_hashes</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># 找到最长前缀匹配</span></span><br><span class="line">        computed_blocks = (</span><br><span class="line">            <span class="variable language_">self</span>.specialized_manager.find_longest_cache_hit(block_hashes))</span><br><span class="line">        <span class="variable language_">self</span>.prefix_cache_stats.queries += <span class="built_in">len</span>(block_hashes)</span><br><span class="line">        <span class="variable language_">self</span>.prefix_cache_stats.hits += <span class="built_in">len</span>(computed_blocks)</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> computed_blocks, num_computed_tokens</span><br></pre></td></tr></table></figure><h4 id="Store"><a href="#Store" class="headerlink" title="Store"></a>Store</h4><p>“vllm&#x2F;v1&#x2F;core&#x2F;block_pool.py”</p><p>在”BlockPool”类中定义了KVCacheBlocks的管理方法</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 传入Request和算好的block，存入BlockPool中</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cache_full_blocks</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        request: Request,</span></span><br><span class="line"><span class="params">        blocks: <span class="built_in">list</span>[KVCacheBlock],</span></span><br><span class="line"><span class="params">        block_hashes: <span class="built_in">list</span>[BlockHashType],</span></span><br><span class="line"><span class="params">        num_cached_blocks: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        num_full_blocks: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        block_size: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        hash_fn: <span class="type">Callable</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Cache a list of full blocks for prefix caching.</span></span><br><span class="line"><span class="string">        This function takes a list of blocks that will have their block hash</span></span><br><span class="line"><span class="string">        metadata to be updated and cached. Given a request, it computes the</span></span><br><span class="line"><span class="string">        block hashes for the blocks starting from `num_cached_blocks` to</span></span><br><span class="line"><span class="string">        `num_full_blocks`, updating the metadata for each block</span></span><br><span class="line"><span class="string">        and caching them in the `cached_block_hash_to_block`.</span></span><br></pre></td></tr></table></figure><h4 id="Eviction"><a href="#Eviction" class="headerlink" title="Eviction"></a>Eviction</h4><p>发生在”BlockPool”类中的<code>get_new_blocks</code>函数中</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_new_blocks</span>(<span class="params">self, num_blocks: <span class="built_in">int</span></span>) -&gt; <span class="built_in">list</span>[KVCacheBlock]:</span><br><span class="line">        ...</span><br><span class="line">            curr_block = <span class="variable language_">self</span>.free_block_queue.popleft()</span><br><span class="line">            ...</span><br><span class="line">            <span class="comment"># 开启了enable_caching且如果没有空闲的block，需要evict掉旧的block</span></span><br><span class="line">            <span class="comment"># If the block is cached, evict it.</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.enable_caching:</span><br><span class="line">                <span class="variable language_">self</span>._maybe_evict_cached_block(curr_block)</span><br><span class="line"></span><br><span class="line">            curr_block.incr_ref()</span><br><span class="line">            ret.append(curr_block)</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_maybe_evict_cached_block</span>(<span class="params">self, block: KVCacheBlock</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        If a block is cached in `cached_block_hash_to_block`, we reset its hash</span></span><br><span class="line"><span class="string">        metadata and evict it from the cache.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            block: The block to evict.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            True if the block is evicted, False otherwise.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        block_hash = block.block_hash</span><br><span class="line">        <span class="keyword">if</span> block_hash <span class="keyword">and</span> block_hash <span class="keyword">in</span> <span class="variable language_">self</span>.cached_block_hash_to_block:</span><br><span class="line">            block.reset_hash()</span><br><span class="line">            <span class="keyword">del</span> <span class="variable language_">self</span>.cached_block_hash_to_block[block_hash][block.block_id]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.cached_block_hash_to_block[block_hash]) == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">del</span> <span class="variable language_">self</span>.cached_block_hash_to_block[block_hash]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><p><code>free_block_queue.popleft()</code>是在”vllm&#x2F;v1&#x2F;core&#x2F;kv_cache_utils”的<code>FreeKVCacheBlockQueue</code>类中使用双向链表实现LRU的（经典leetcode题）</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FreeKVCacheBlockQueue</span>:</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">popleft</span>(<span class="params">self</span>) -&gt; KVCacheBlock:</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">remove</span>(<span class="params">self, block: KVCacheBlock</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">append</span>(<span class="params">self, block: KVCacheBlock</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_all_free_blocks</span>(<span class="params">self</span>) -&gt; <span class="built_in">list</span>[KVCacheBlock]:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><h3 id="🔋-5-KVCache感知的多实例路由"><a href="#🔋-5-KVCache感知的多实例路由" class="headerlink" title="🔋 5. KVCache感知的多实例路由"></a>🔋 5. KVCache感知的多实例路由</h3><h4 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h4><p>使用字符串匹配而不是基于 token-ID 的匹配。</p><ul><li>tokenization（分词）本身相当慢（需要几微秒），所以对每个请求都执行会带来巨大的开销。</li><li>实现字符串服务器（例如使用 Redis）作为存储后端。</li></ul><h4 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h4><p>路由器可以向 KV 缓存管理系统发送请求：哪个服务器有最长的匹配前缀？<br>vLLM production stack 团队的解决方案：<br><a href="https://github.com/vllm-project/production-stack/issues/59">https://github.com/vllm-project/production-stack/issues/59</a></p><h4 id="KVcache-aware-VS-Load-balance"><a href="#KVcache-aware-VS-Load-balance" class="headerlink" title="KVcache aware VS Load balance"></a>KVcache aware VS Load balance</h4><p>这里有个trade-off, KV 缓存感知路由可能会将请求路由到同一个节点，这对负载均衡不利。</p>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
            <tag> vLLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EP04-vLLM源码讲解直播笔记-Speculative Decoding</title>
      <link href="/2025/04/05/vLLM-EP04/"/>
      <url>/2025/04/05/vLLM-EP04/</url>
      
        <content type="html"><![CDATA[<h1 id="FIXME-EP04-vLLM-源码讲解直播笔记"><a href="#FIXME-EP04-vLLM-源码讲解直播笔记" class="headerlink" title="[FIXME][EP04] vLLM 源码讲解直播笔记"></a>[FIXME][EP04] vLLM 源码讲解直播笔记</h1><h2 id="EP04-Speculative-Decoding"><a href="#EP04-Speculative-Decoding" class="headerlink" title="EP04: Speculative Decoding"></a>EP04: Speculative Decoding</h2><p>直播回看链接：<a href="https://www.youtube.com/watch?v=WF5xaQqtKUE">https://www.youtube.com/watch?v=WF5xaQqtKUE</a></p><p>特别鸣谢：月球大叔，Du Kuntai, Cheng Yihua 大佬带来的精彩讲解</p><h3 id="📌-1-为什么需要-Speculative-decoding（推测解码）"><a href="#📌-1-为什么需要-Speculative-decoding（推测解码）" class="headerlink" title="📌 1. 为什么需要 Speculative decoding（推测解码）"></a>📌 1. 为什么需要 Speculative decoding（推测解码）</h3><ul><li><p>LLM的decode过程是GPU-Memory-Bound (GPU内存受限) 的</p><ul><li>寻找一种方法能够增加计算次数，但不显著增加对GPU内存的访问次数</li></ul></li><li><p>解决方法：在decode生成token的时候 –&gt; 用小模型猜多几个token并验证</p><ul><li>在 token 生成的每1次迭代中<ul><li>猜3个token，接受率为2&#x2F;3</li><li>2个token是猜测正确的，LLM推理每次还会生成1个新的token –&gt; 3 tokens</li></ul></li><li>一次的迭代所需要的时间<ul><li>计算量：(1 + 3)x</li><li>内存量<ul><li>没有 Speculative decoding 时：模型参数（8x2 GB）+ KVCache（n * 100 KB）</li><li>有 Speculative decoding 时：模型参数（8x2 GB）+ KVCache（（n+3） * 100 KB）</li></ul></li><li>一次迭代的时间不变，吞吐量增加3倍</li></ul></li></ul></li><li><p>衡量一个操作是 computation-bound（计算受限型）还是 memory-bound（内存受限型）的指标是 arithmetic intensity（计算强度）：</p><ul><li>定义：FLOPS（每秒浮点运算次数）&#x2F; MIPs（内存指令次数）</li></ul></li><li><p>Speculative decoding虽然是个很好的优化点，但在实际落地的过程中还面临很多工程上的困难</p></li></ul><h3 id="⚡-2-怎么猜测-token"><a href="#⚡-2-怎么猜测-token" class="headerlink" title="⚡ 2. 怎么猜测 token?"></a>⚡ 2. 怎么猜测 token?</h3><ul><li><p>N-gram</p><ul><li><p>构造一个mapping：如果前3个tokens是A, B, C，接下来两个tokens是D, E</p></li><li><p>示例：</p><ul><li>如果前3个tokens是 <code>To be or</code>，接下来两个tokens是 <code>not to</code></li><li>如果前3个tokens是 <code>be or not</code>，接下来两个tokens是 <code>to be</code></li><li>…</li><li>如果前3个tokens是 <code>, this is</code>，接下来两个tokens是 <code>a question</code></li></ul></li><li><p>从请求输入中构建N-gram，使用这个N-gram来猜测tokens：</p><ul><li><p>接下来是莎士比亚的一些名言：</p><p>  …<br>  <code>To be or not to be, this is a question</code><br>  …</p></li></ul><p>  里面最经典的一句名言是什么?</p><ul><li>假设LLM已经生成：<ul><li><code>Sure! We recommend you this quote: &quot;To be or</code></li><li>猜测：接下来的两个tokens是 <code>not to</code></li><li>验证：正确</li><li>输出：<code>Sure! We recommend you this quote: &quot;To be or not to be&quot;</code></li></ul></li></ul></li></ul></li><li><p>Model-based（draft model）</p><ul><li>Parallel guessing（并行猜测）<ul><li>优点：快</li><li>缺点：在猜测第二个token的时候不知道第一个token是什么</li></ul></li><li>Autoregression guessing（自回归猜测）<ul><li>优点：在猜测第二个token的时候知道第一个token</li><li>缺点：慢</li></ul></li></ul></li><li><p>Deployment（尤其是model-based）存在的问题</p><ul><li>小模型需要KVCache，应该怎么放置？</li><li>小模型小，需要不同的并行策略<ul><li>假设小模型不并行，小模型在0号GPU + vLLM强制不同的GPU有一致的GPU内存利用率（同一个并行组内）–&gt; 会造成其他GPU内存的浪费</li></ul></li><li>要为guessed tokens提前allocate KVCache<ul><li>如果allocate KVCache要跨越vLLM的block边界怎么办</li><li>需要discard的token</li></ul></li><li>从 Sampling –&gt; verification 阶段的转变</li><li>最小化 overhead (Ngram)</li><li>怎么确定每一次应该guess多少个tokens</li><li>怎么在不同requests之间区分<ul><li>不同的request：不同的token数量，它们其中的一部分不进行spec decode</li></ul></li></ul></li></ul><h3 id="📍-3-怎么验证-token-的正确性"><a href="#📍-3-怎么验证-token-的正确性" class="headerlink" title="📍 3. 怎么验证 token 的正确性?"></a>📍 3. 怎么验证 token 的正确性?</h3><ul><li><p>Tree verification（树验证）</p><ul><li><code>To be or</code> –&gt; <code>not to</code>, <code>sleep in</code>, <code>go to</code></li><li><code>To be or</code> 有很多种猜法: <code>not to</code>, <code>sleep in</code>, <code>go to</code></li></ul></li><li><p>LLM怎么验证预测的是正确的?</p><ul><li>Deterministic sampling（确定性采样）(spec decode bad case)</li><li>Random sampling（随机性采样），当 guess probability &gt; threshold 就正确</li></ul></li><li><p>示例：</p><ul><li><p>输入：<code>To be or</code> (already-decoded output) <code>not to</code> (guessed token)</p><ul><li><code>To be or not to</code></li><li><code>To -&gt; be</code></li><li><code>be -&gt; or</code></li><li><code>or -&gt; not</code> 我们的猜测 “not” 是正确的</li><li><code>not -&gt; to</code> 我们的猜测 “to” 是正确的</li><li><code>to -&gt; be</code> “be” 是正确的下一个token</li></ul></li><li><p>输入：<code>To be or</code> (already-decoded output) <code>not be</code> (guessed token)</p><ul><li><code>To be or not be</code></li><li><code>To -&gt; be</code></li><li><code>be -&gt; or</code></li><li><code>or -&gt; not</code> 我们的猜测 “not” 是正确的</li><li><code>not -&gt; be</code> 我们的猜测 “be” 是错误的，应该是 “to”<br>  发现有错误的token后，后面的预测会舍弃</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
            <tag> vLLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EP03-vLLM源码讲解直播笔记-PD分离</title>
      <link href="/2025/03/30/vLLM-EP03/"/>
      <url>/2025/03/30/vLLM-EP03/</url>
      
        <content type="html"><![CDATA[<h1 id="FIXME-EP03-vLLM-源码讲解直播笔记"><a href="#FIXME-EP03-vLLM-源码讲解直播笔记" class="headerlink" title="[FIXME][EP03] vLLM 源码讲解直播笔记"></a>[FIXME][EP03] vLLM 源码讲解直播笔记</h1><h2 id="EP03-PD分离"><a href="#EP03-PD分离" class="headerlink" title="EP03: PD分离"></a>EP03: PD分离</h2><p>直播回看链接：<a href="https://www.youtube.com/watch?v=ih6fcJnhoJI">https://www.youtube.com/watch?v=ih6fcJnhoJI</a></p><p>特别鸣谢：月球大叔，Cheng Yihua 大佬带来的精彩讲解</p><h3 id="📌-1-上周回顾（分布式通信与并行策略）"><a href="#📌-1-上周回顾（分布式通信与并行策略）" class="headerlink" title="📌 1. 上周回顾（分布式通信与并行策略）"></a>📌 1. 上周回顾（分布式通信与并行策略）</h3><ul><li>TP，all_gather<ul><li>Linear M x N x K -&gt; M x K</li><li>M * N’, N’ * K -&gt; M * K<br>  在MHA中，每个头被均匀地分在不同的worker上，在进入之后的线性层前要做一次all_gather（这里不理解的可以看看-lm张量并行的方法）</li><li>“vllm\model_executor\models\llama.py”  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># llama前向传播的代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    positions: torch.Tensor,</span></span><br><span class="line"><span class="params">    hidden_states: torch.Tensor,</span></span><br><span class="line"><span class="params"></span>) -&gt; torch.Tensor:</span><br><span class="line">    qkv, _ = <span class="variable language_">self</span>.qkv_proj(hidden_states)</span><br><span class="line">    q, k, v = qkv.split([<span class="variable language_">self</span>.q_size, <span class="variable language_">self</span>.kv_size, <span class="variable language_">self</span>.kv_size], dim=-<span class="number">1</span>)</span><br><span class="line">    q, k = <span class="variable language_">self</span>.rotary_emb(positions, q, k)</span><br><span class="line">    attn_output = <span class="variable language_">self</span>.attn(q, k, v)</span><br><span class="line">    output, _ = <span class="variable language_">self</span>.o_proj(attn_output)</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="⚡-2-PD分离"><a href="#⚡-2-PD分离" class="headerlink" title="⚡ 2. PD分离"></a>⚡ 2. PD分离</h3><ul><li>什么是PD分离（Prefill和Decode）<ul><li>prefill: 处理输入的prompt，生成KVCache</li><li>decode: 根据KVCache连续自回归生成一个一个的token</li></ul></li><li>为什么要PD分离<ul><li>prefill: attention N tokens QKV，与序列长度n的平方成正比，需要相当多时间</li><li>decode: attention N KV, 1Q，生成新的token，速度较快</li><li>最初的逻辑：prefill优先</li><li>问题：当新的一个request到来时，进行的prefill会使其他正在decode的request停住</li><li>解决方法：<ul><li>PD分离（PD disaggregation）<ul><li>挑战是P，D的数量</li></ul></li><li>分块预填充（chunked prefill），已在vllm v1版本中默认使用<ul><li>挑战是chunked_size的设置，这里有一个prefill和decode中的trade-off</li></ul></li></ul></li></ul></li><li>PD分离的关键问题<ul><li>怎么传输KVCache<ul><li>两种模式：pooling模式，P2P模式</li><li>LMCache都支持上面两种模式，Mooncake(pooling)，NIXL(p2p)</li></ul></li><li>怎么从vllm提取（注入）KVCache<ul><li>connector API<ul><li>在model_runner中被调用，”vllm\worker\model_runner.py”</li><li>在模型forward前：尝试接收KVCache并注入到到vllm的pages memory中</li><li>模型执行</li><li>在模型forward后，将KVCache从pages memory中并将它发送出去<br><img src="/posts_img/vLLM-EP03/1743330247803.png"><ul><li>两个函数的详细设计在”vllm\distributed\kv_transfer\kv_transfer_agent.py”中  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 本质上是根据model input计算出KVCache放在page memory中的什么地方</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">recv_kv_caches_and_hidden_states</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self, model_executable: torch.nn.Module,</span></span><br><span class="line"><span class="params">    model_input: <span class="string">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span>,</span></span><br><span class="line"><span class="params">    kv_caches: <span class="type">List</span>[torch.Tensor]</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Union</span>[torch.Tensor, IntermediateTensors], <span class="built_in">bool</span>,</span><br><span class="line">        <span class="string">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span>]:</span><br><span class="line">              </span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.connector.recv_kv_caches_and_hidden_states(</span><br><span class="line">        model_executable, model_input, kv_caches)</span><br></pre></td></tr></table></figure>  这里不懂的可以去看”vllm\distributed\kv_transfer\kv_connector\simple_connector.py”</li></ul></li></ul></li></ul></li><li>什么时候将request从P node传输到D node<ul><li>先P后D（production stack）</li><li>先D后P（D node收到后先检查是否有KVCache，没有的话再转给P node去做，这个思路主要考虑的是TTFT）</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
            <tag> vLLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CacheBlend-高效提高KVCache复用性的方法</title>
      <link href="/2025/03/29/CacheBlend/"/>
      <url>/2025/03/29/CacheBlend/</url>
      
        <content type="html"><![CDATA[<p>CacheBlend，一种用于加速 LLM 服务的高效KV缓存融合方案，特别针对检索增强生成（RAG）等需要多文本块上下文的场景，能有效提高 KVCache 的复用性。</p><p>论文：(EuroSys 2025)<a href="https://arxiv.org/abs/2405.16444">Fast Large Language Model Serving for RAG with Cached Knowledge Fusion</a></p><p>代码：<a href="https://github.com/YaoJiayi/CacheBlend">https://github.com/YaoJiayi/CacheBlend</a></p><h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>为了确保模型高质量的一致性的响应，RAG技术常被应用于LLM服务中，在这种情况下，多个从数据库检索来的文本片段会前置于用户的查询，形成LLM输入，这些长上下文片段显著地提高了LLM的TTFT时间。</p><p>为了提高prefill的时间，现有的优化通常重复使用存储的 KVCache，以避免重复的计算。目前有以下两类 KVCache重用的方法，但它们都存在局限性。</p><h2 id="前缀缓存"><a href="#前缀缓存" class="headerlink" title="前缀缓存"></a>前缀缓存</h2><p>仅能重用输入前缀的KV缓存，无法处理多文本块场景。</p><h2 id="全量KV重用"><a href="#全量KV重用" class="headerlink" title="全量KV重用"></a>全量KV重用</h2><p>当重用的文本不在输入前缀中时，仍然通过调整其位置嵌入来重用 KV 缓存（这里用到了 Rotary Position Embedding(RoPE) 编码相对位置的特性），从而使 LLM 生成有意义的输出（Prompt cache 2023）。但这样会忽略与之前文本块之间的<strong>跨注意力</strong>（cross-attention），导致生成质量显著下降。（如果这里不理解什么是cross-attention的话，可以重新回顾一下整个prefill阶段的计算流，tensor在不同的layer前向传递过程中，上下文的内容会不断融合）</p><p><img src="/posts_img/CacheBlend/2.png"></p><p>这个图表明了：</p><ol><li>随着选择的文本片段数量增加，生成质量显著提高，尽管包括在太多片段时会由于lost-in-the-middle问题而降低质量。</li><li>随着选择的文本片段数量增加，由于交叉注意力缺失造成的性能损耗增大。</li></ol><table><thead><tr><th align="center"><img src="/posts_img/CacheBlend/3.png" alt="Image 1"></th><th align="center"><img src="/posts_img/CacheBlend/4.png" alt="Image 2"></th></tr></thead></table><p>左图表示了，由于忽略交叉注意力可能导致错误的回答。为了理解这一点，可以仔细看一下两个文本片段之间的交叉注意力。可以看到，相比于完整prefill后的结果，全量KVCache复用时两个片段之间的交叉注意力被忽略了。</p><p>这里要说明一个点，当各片段之间的交叉注意力较低时，片段内自注意力影响较高时，完全重用 KV 是可以奏效的。这种情况在 PromptCache 的主要目标应用（gim2023prompt）的提示模板中较为常见。</p><h2 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h2><p>如何在重用非前缀文本块的KV缓存时，保持生成质量与完全预填充（full prefill）一致，同时减少计算延迟？</p><p><img src="/posts_img/CacheBlend/1.png"></p><p>作者在这里做了一个trade-off，的核心思想是在 Full KV Reuse 的基础上再计算少量 token 的注意力以恢复文本块之间的交叉注意力。</p><h1 id="核心方法：选择性KV重计算"><a href="#核心方法：选择性KV重计算" class="headerlink" title="核心方法：选择性KV重计算"></a>核心方法：选择性KV重计算</h1><h2 id="选择性KV重计算（Selective-KV-Recomputation）"><a href="#选择性KV重计算（Selective-KV-Recomputation）" class="headerlink" title="选择性KV重计算（Selective KV Recomputation）"></a>选择性KV重计算（Selective KV Recomputation）</h2><ul><li><strong>目标</strong>：仅更新对生成质量影响最大的部分token的KV缓存，避免全量预填充的计算开销。</li><li><strong>实现步骤</strong>：在每一层（Layer）的输入中，通过掩码仅保留需要重计算的token（HKVD tokens），计算所需的QKV，其余token的KV值直接复用预计算的缓存。</li></ul><p><img src="/posts_img/CacheBlend/5.png"></p><h2 id="重计算Tokens的筛选"><a href="#重计算Tokens的筛选" class="headerlink" title="重计算Tokens的筛选"></a>重计算Tokens的筛选</h2><p>知道了要重计算关键token，那么如何选择这些token呢？</p><ul><li><strong>定义</strong>：<strong>高KV偏差（High KV Deviation, HKVD）Token</strong><br>指其预计算KV值与完全预填充（Full KV Recompute）结果的差异超过阈值的token。  <ul><li><strong>量化指标</strong>：<br>$$\Delta_{\text{kv}}(KV_i[j], KV_i^{\text{full}}[j]) &#x3D; |KV_i[j] - KV_i^{\text{full}}[j]|$$<br>其中，$KV_i[j]$ 表示第i层第j个token的KV值。</li></ul></li></ul><h2 id="筛选依据"><a href="#筛选依据" class="headerlink" title="筛选依据"></a>筛选依据</h2><h3 id="注意力稀疏性"><a href="#注意力稀疏性" class="headerlink" title="注意力稀疏性"></a>注意力稀疏性</h3><p>仅约10-15%的token的KV偏差远高于其他令牌，对跨文本块注意力贡献显著。<br><img src="/posts_img/CacheBlend/6.png"></p><h3 id="层间相关性"><a href="#层间相关性" class="headerlink" title="层间相关性"></a>层间相关性</h3><p>如何在不知道真实 KV 值或注意力矩阵的情况下识别 HKVD 令牌？简单来说，要识别 HKVD 令牌，首先必须知道每一层的完全重新计算的 KV，但这太过昂贵且违背了选择性 KV 重新计算的目的。相反，作者观察到不同层的 HKVD 令牌并不是独立的！</p><p>通过Spearman秩相关性分析，发现相邻层的HKVD tokens高度重叠，允许跨层渐进筛选，这是因为各层之间的 KV 缓存具有相似性。</p><p><img src="/posts_img/CacheBlend/7.png"></p><h3 id="渐进式筛选流程（Gradual-Filtering）"><a href="#渐进式筛选流程（Gradual-Filtering）" class="headerlink" title="渐进式筛选流程（Gradual Filtering）"></a>渐进式筛选流程（Gradual Filtering）</h3><p>如果平均每层想要挑选 $r$% 个 HKVD 标记：</p><ol><li><strong>首层筛选</strong>：在第一层预填充时，计算所有token的KV偏差，选择偏差最高的前$r_1$%作为候选，例如20%）。  </li><li><strong>逐层过滤</strong>：  <ul><li>对后续每一层，仅计算上一层的候选token的KV偏差，从中筛选偏差更高的(r_i%)（逐步逼近目标比例(r)）。  </li><li>最终每层仅重计算约10-15%的token，显著减少计算量。</li></ul></li></ol><p><img src="/posts_img/CacheBlend/8.png"></p><h1 id="系统优化：延迟隐藏与动态控制"><a href="#系统优化：延迟隐藏与动态控制" class="headerlink" title="系统优化：延迟隐藏与动态控制"></a>系统优化：延迟隐藏与动态控制</h1><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><p><img src="/posts_img/CacheBlend/10.png"></p><h2 id="流水线并行（Pipelining）"><a href="#流水线并行（Pipelining）" class="headerlink" title="流水线并行（Pipelining）"></a>流水线并行（Pipelining）</h2><ul><li><strong>KV加载与重计算并行</strong>：  <ul><li>在GPU计算当前层的选择性KV重计算时，异步加载下一层的预计算KV缓存（从SSD&#x2F;内存到GPU）。  </li><li>若加载时间 ≥ 重计算时间，额外延迟被完全隐藏。</li></ul></li><li><strong>优势</strong>：支持将KV缓存存储在低速设备（如SSD）中，节省内存成本，同时不影响实时性。</li></ul><p><img src="/posts_img/CacheBlend/11.png"></p><h2 id="动态控制器（Loading-Controller）"><a href="#动态控制器（Loading-Controller）" class="headerlink" title="动态控制器（Loading Controller）"></a>动态控制器（Loading Controller）</h2><ul><li><strong>动态调整重计算比例</strong>：  <ul><li>根据存储设备速度（如SSD吞吐量）和模型规模，计算最大可容忍的重计算比例(r%)，使得：<br>$$T_{\text{recompute}}(r%) \leq T_{\text{load}}(storage_device)$$  </li><li>确保总延迟不增加，同时质量损失可控（经验默认r &#x3D; 15%)）。</li></ul></li><li><strong>存储设备选择</strong>：<br>在成本与延迟间权衡，优先选择满足延迟约束的最廉价存储设备（如SSD而非GPU显存）。</li></ul><p><img src="/posts_img/CacheBlend/9.png"></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><strong>实验结果</strong></h1><ul><li><strong>性能提升</strong>：  <ul><li><strong>服务质量</strong>：相比完全预填充，首token时间（TTFT）减少 <strong>2.2-3.3倍</strong>，吞吐量提升 <strong>2.8-5倍</strong>。  </li><li><strong>生成质量</strong>：与完全预填充相比，F1&#x2F;Rouge-L分数下降小于 <strong>0.02</strong>，显著优于完全KV重用（质量提升最高达0.35）。</li></ul></li><li><strong>适用性</strong>：在多个模型（Mistral-7B、Yi-34B、Llama-70B）和任务（QA、摘要）中验证有效性，支持不同文本块长度和批量大小。</li></ul><p><img src="/posts_img/CacheBlend/12.png"></p><h1 id="贡献与意义"><a href="#贡献与意义" class="headerlink" title="贡献与意义"></a><strong>贡献与意义</strong></h1><ul><li><strong>理论价值</strong>：揭示了注意力稀疏性与KV偏差的关系，为高效缓存复用提供了新视角。  </li><li><strong>工程价值</strong>：通过流水线和动态控制，实现了低成本、高质量的KV缓存复用，兼容现有LLM服务框架（如vLLM）。  </li><li><strong>应用场景</strong>：适用于需要多上下文交互的RAG、长文本生成等任务，显著降低服务延迟与计算开销。</li></ul><h1 id="局限与展望"><a href="#局限与展望" class="headerlink" title="局限与展望"></a><strong>局限与展望</strong></h1><ul><li>目前仅验证了Transformer架构，未来需适配Mamba等新型模型。  </li><li>可进一步结合KV压缩技术（如量化、剪枝）减少存储开销。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> EuroSys 2025 </tag>
            
            <tag> Inference </tag>
            
            <tag> KVCache </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EP02-vLLM源码讲解直播笔记-分布式通信与并行策略</title>
      <link href="/2025/03/25/vLLM-EP02/"/>
      <url>/2025/03/25/vLLM-EP02/</url>
      
        <content type="html"><![CDATA[<h1 id="FIXME-EP02-vLLM-源码讲解直播笔记"><a href="#FIXME-EP02-vLLM-源码讲解直播笔记" class="headerlink" title="[FIXME][EP02] vLLM 源码讲解直播笔记"></a>[FIXME][EP02] vLLM 源码讲解直播笔记</h1><h2 id="EP02-分布式通信与并行策略"><a href="#EP02-分布式通信与并行策略" class="headerlink" title="EP02: 分布式通信与并行策略"></a>EP02: 分布式通信与并行策略</h2><p>直播回看链接：<a href="https://www.youtube.com/watch?v=W83Zgbg8SkE&t=4s">https://www.youtube.com/watch?v=W83Zgbg8SkE&amp;t=4s</a></p><p>特别鸣谢：月球大叔，Du Kuntai，Cheng Yihua 大佬带来的精彩讲解</p><h3 id="📌-1-GroupCoordinator-类解析"><a href="#📌-1-GroupCoordinator-类解析" class="headerlink" title="📌 1. GroupCoordinator 类解析"></a>📌 1. GroupCoordinator 类解析</h3><p>‘vllm&#x2F;distributed&#x2F;parallel_state.py’</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以把GroupCoordinator想象成一个群聊</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GroupCoordinator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    # PyTorch ProcessGroup 的封装，管理进程组间的通信</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    PyTorch ProcessGroup wrapper for a group of processes.</span></span><br><span class="line"><span class="string">    PyTorch ProcessGroup is bound to one specific communication backend,</span></span><br><span class="line"><span class="string">        e.g. NCCL, Gloo, MPI, etc.</span></span><br><span class="line"><span class="string">    GroupCoordinator takes charge of all the communication operations among</span></span><br><span class="line"><span class="string">        the processes in the group. It can route the communication to</span></span><br><span class="line"><span class="string">        a specific implementation (e.g. switch allreduce implementation</span></span><br><span class="line"><span class="string">        based on the tensor size and cuda graph mode).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># available attributes:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 这个群里表示的是我是谁</span></span><br><span class="line">    rank: <span class="built_in">int</span>  <span class="comment"># global rank</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在这个群里加上我还有哪些人</span></span><br><span class="line">    ranks: <span class="type">List</span>[<span class="built_in">int</span>]  <span class="comment"># global ranks in the group</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在群里的人数</span></span><br><span class="line">    world_size: <span class="built_in">int</span>  <span class="comment"># size of the group</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># difference between `local_rank` and `rank_in_group`:</span></span><br><span class="line">    <span class="comment"># if we have a group of size 4 across two nodes:</span></span><br><span class="line">    <span class="comment"># Process | Node | Rank | Local Rank | Rank in Group</span></span><br><span class="line">    <span class="comment">#   0     |   0  |  0   |     0      |       0</span></span><br><span class="line">    <span class="comment">#   1     |   0  |  1   |     1      |       1</span></span><br><span class="line">    <span class="comment">#   2     |   1  |  2   |     0      |       2</span></span><br><span class="line">    <span class="comment">#   3     |   1  |  3   |     1      |       3</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对应逻辑上的GPU id（比如一台8卡机上，0号GPU被占用了的话，SET_CUDA_DEVICE=1-6之后，内部映射为0-5）</span></span><br><span class="line">    local_rank: <span class="built_in">int</span>  <span class="comment"># local rank used to assign devices</span></span><br><span class="line"></span><br><span class="line">    rank_in_group: <span class="built_in">int</span>  <span class="comment"># rank inside the group</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># CPU的通信更加可控，更好做，用于主机端同步（如初始化阶段），支持任意通信后端（Gloo/MPI）</span></span><br><span class="line">    cpu_group: ProcessGroup  <span class="comment"># group for CPU communication</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用于设备间数据传输（必须使用支持GPU的后端，如NCCL）</span></span><br><span class="line">    device_group: ProcessGroup  <span class="comment"># group for device communication</span></span><br><span class="line"></span><br><span class="line">    use_pynccl: <span class="built_in">bool</span>  <span class="comment"># a hint of whether to use PyNccl</span></span><br><span class="line">    use_custom_allreduce: <span class="built_in">bool</span>  <span class="comment"># a hint of whether to use CustomAllreduce</span></span><br><span class="line">    <span class="comment"># communicators are only created for world size &gt; 1</span></span><br><span class="line">    pynccl_comm: <span class="type">Optional</span>[<span class="type">Any</span>]  <span class="comment"># PyNccl communicator</span></span><br><span class="line">    ca_comm: <span class="type">Optional</span>[<span class="type">Any</span>]  <span class="comment"># Custom allreduce communicator</span></span><br><span class="line">    mq_broadcaster: <span class="type">Optional</span>[<span class="type">Any</span>]  <span class="comment"># shared memory broadcaster</span></span><br></pre></td></tr></table></figure><h3 id="⚡-2-并行策略详解"><a href="#⚡-2-并行策略详解" class="headerlink" title="⚡ 2. 并行策略详解"></a>⚡ 2. 并行策略详解</h3><ul><li><p>TP（张量并行）</p><ul><li>需要allreduce，通信量大，对于通信需求较高</li><li>Infra端<ul><li>通信设备<ul><li>NVLink: GPU之间的直接通信，常用于节点内通信</li><li>InfiniBand: 本质上也是硬件，常用于节点间通信</li><li>RDMA: RDMA网卡，最大的好处是跳过操作系统 &#x2F; zero copy, RoCE</li></ul></li><li>通信库：’vllm&#x2F;distributed&#x2F;device_communicators’<ul><li>PyNccl: Nvidia 之间的通信</li><li>Shared memory: 操作系统中不同进程之间数据共享</li><li>Custom allreduce: 专为all reduce操作的kernel </li><li>torch.distributed: 广泛支持一系列的通信库</li></ul></li></ul></li><li>算法端<ul><li>想了解任何通信方式可以了解：’vllm&#x2F;model_executor&#x2F;models&#x2F;llama.py’，llama系模型支持各种并行方式，适合初学者学习架构</li><li>‘get_tp_group()’</li></ul></li></ul></li><li><p>PP（流水线并行）</p><ul><li>通信量相对较小，对device–device通信需求较低</li><li>不能降低延迟，但能提高吞吐</li><li>算法端<ul><li>每个worker负责一个layers的子集<ul><li>‘vllm&#x2F;model_executor&#x2F;models&#x2F;llama.py’ 中 self.start_layer –&gt; self.end_layer</li><li>在worker之间: communicate IntermediateTensor</li><li>‘vllm&#x2F;worker&#x2F;model_runner.py’: 搜索 ‘get_pp_group()’</li></ul></li></ul></li></ul></li><li><p>EP（专家并行）&amp; DP（数据并行）</p><ul><li><p>为什么要有EP？</p><ul><li>Mistral &#x2F; Mixtral &#x2F; Deepseek 都是用MOE</li><li>MOE具有计算稀疏性，每个request只激活一小部分的expert</li></ul></li><li><p>将不同的expert放在不同的device上–&gt;专家并行</p></li><li><p>算法端</p><ul><li>Shuffle (DeepEP communication kernel)</li><li>Forward</li><li>Shuffle back</li></ul></li><li><p>在Attention模块做TP，在FFN模块做EP</p></li><li><p>share expert负载较高，要做冗余</p></li><li><p>DP (数据并行)</p><ul><li>最大的TP &lt;&lt; 所需要的EP（EP&#x3D;320）</li><li>TP &lt; # attention head</li><li>TP * DP &#x3D;&#x3D; EP（通过请求并行的方式去拉满计算资源）</li><li>在实践中难以应用<ul><li>对请求进行padding避免造成死锁</li></ul></li></ul></li></ul></li></ul><p>​    </p>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
            <tag> vLLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sarathi-Serve-PD融合的LLM服务调度器</title>
      <link href="/2025/03/19/Sarathi/"/>
      <url>/2025/03/19/Sarathi/</url>
      
        <content type="html"><![CDATA[<p>Sarathi-Serve 是一个高效大型语言模型（LLM）推理调度器，旨在解决LLM推理中<strong>吞吐量</strong>和<strong>延迟</strong>之间的权衡问题。通过引入“分块预填充（chunked-prefills）”和“无停顿调度（stall-free scheduling）”技术，Sarathi-Serve能够在保持低延迟的同时显著提高推理吞吐量。</p><p>论文：(OSDI 2024)<a href="https://www.usenix.org/system/files/osdi24-agrawal.pdf">Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve</a></p><p>代码：<a href="https://github.com/microsoft/sarathi-serve">microsoft&#x2F;sarathi-serve: A low-latency &amp; high-throughput serving engine for LLMs</a></p><p>该文章参考自：<a href="https://zhuanlan.zhihu.com/p/12679786211">https://zhuanlan.zhihu.com/p/12679786211</a></p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="LLM服务特性"><a href="#LLM服务特性" class="headerlink" title="LLM服务特性"></a>LLM服务特性</h2><p>LLM推理分为两个阶段：</p><ul><li><strong>Prefill（预填充）</strong>：处理输入提示并生成首个输出令牌，计算密集但延迟高。</li><li><strong>Decode（解码）</strong>：逐个生成后续令牌，延迟低但计算利用率低。</li></ul><p>当前的LLM推理调度器大致可以分为两类，即根据它们在批处理请求时如何调度预填充和解码阶段，分为<strong>预填充优先</strong>和<strong>解码优先</strong>：</p><ol><li><p><strong>传统的请求级批处理系统，如FasterTransformer，采用解码优先的调度策略。</strong></p><p> 只有批处理中的所有请求都完成了它们的解码阶段后，该批处理才算完成，即只要有一个或多个请求正在进行解码，就不会调度新的预填充。</p></li><li><p><strong>现有调度策略（如vLLM、Orca）引入迭代级调度，使用连续批处理技术。</strong></p><p> 每当GPU内存可用时，优先调度预填充阶段的请求。预填充优先的调度器具有更好的吞吐量，因为这样允许后续的解码以高批次大小运行。然而，优先处理预填充会干扰正在进行的解码，导致高延迟（生成停滞）。</p></li></ol><p>总的来说，现有LLM服务存在以下问题：</p><ul><li>吞吐量与延迟的冲突：优先处理Prefill（提高吞吐量）会导致解码阶段的高延迟（生成停滞），而优先处理Decode（降低延迟）则会牺牲吞吐量。</li><li>管道并行中的气泡：混合Prefill和Decode的批次因计算时间差异导致GPU资源浪费。</li><li>长提示处理效率低：长输入提示的Prefill阶段耗时过长，加剧延迟波动。</li></ul><h1 id="存在的问题与挑战"><a href="#存在的问题与挑战" class="headerlink" title="存在的问题与挑战"></a>存在的问题与挑战</h1><ul><li><p>预填充和解码阶段的成本分析。</p><p><img src="/posts_img/Sarathi/1.png"></p><p>LLM推理的两个阶段——预填充和解码——表现出截然相反的行为，其中批量处理可以极大地提高解码阶段的吞吐量，但对预填充吞吐量几乎没有影响；序列长度极大地影响预填充的时间，但批量大小几乎不影响解码的延迟。</p><p>解码批处理在内存受限的状态下运行，导致计算未得到充分利用。这意味着可以在解码批处理的同时处理更多的令牌，而不会显著增加其延迟。</p></li><li><p>优化线性操作对于提高大型语言模型（LLM）的推理效率至关重要。</p><p><img src="/posts_img/Sarathi/2.png"></p><p>从图中我们可以看到，预填充时间随着序列长度增加而近乎二次增长，且线性操作占用了大部分的运行时间成本。虽然注意力成本随着序列长度的增加而呈二次增长，但在高序列长度下，线性操作仍然贡献了超过80%的总时间。</p></li><li><p>预填充和解码阶段的计算特性是不同的。</p></li></ul><table><thead><tr><th align="center"><img src="/posts_img/Sarathi/3.png" alt="Image 1"></th><th align="center"><img src="/posts_img/Sarathi/4.png" alt="Image 2"></th></tr></thead></table><p>  在上图（左）中，用（计算量&#x2F;访存量）衡量两个阶段的计算强度，Decode处于访存受限区域，Prifill处于计算受限区域，最理想的是平衡点是操作的算术强度与设备的FLOPS-to-Bandwidth比率相匹配。</p><p>  右图显示了LLaMA2-70B中<strong>线性层</strong>计算在一次迭代中的总执行时间随token数量的变化。在开始时，当批次处于受内存限制的状态时，执行时间仅略有增加，但随后随着批次变为受计算限制的状态，执行时间呈线性增长。</p><ul><li><p>吞吐量与延迟的权衡。</p><p><img src="/posts_img/Sarathi/5.png"></p><p>vLLM和Orca都是prefill优先，vLLM无法混合pd阶段的batch，Orca通过线性层批处理做到了pd混合batch，但由于prefill阶段的Attention算子耗时长，还是会拖累TBT。FastTransformer是decoding优先，无法混合pd阶段的batch，stall了prefill阶段，导致TTFT大。</p><p>当今最先进的系统使用预填充优先级调度，批越大，吞吐量越高，延迟越高，要根据所需的SLO在吞吐量和延迟之间进行权衡。</p></li><li><p>流水线气泡。</p><p><img src="/posts_img/Sarathi/6.png"></p><p>推理过程中存在的三种类型的气泡：</p><ol><li>由于连续两个微批次中prefill令牌的数量不同而产生；</li><li>由于prefill和decode阶段的计算时间不同而产生；</li><li>由于微批次之间decode计算时间的差异而产生，因为注意力成本取决于累积的上下文长度（KV缓存的大小），并在不同请求之间变化。</li></ol><p>总的来说，是由于微批之间的计算不均匀导致，本质上主要还是因为无法完美耦合prefill和decode两个计算、调度特性的不同的阶段。</p></li></ul><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><h2 id="分块预填充"><a href="#分块预填充" class="headerlink" title="分块预填充"></a>分块预填充</h2><p>允许在多个迭代中以小块形式计算prefill。基于前面的挑战可以发现，适度序列长度的prefill请求可以有效地使GPU计算能力达到饱和，那么利用这一机制形成具有适当token数量的批次，将序列prefill的一部分加入到decode批中来，以充分利用计算潜力，同时不违反TBT（总批处理时间）服务级别目标。</p><h2 id="无停顿批处理"><a href="#无停顿批处理" class="headerlink" title="无停顿批处理"></a>无停顿批处理</h2><p>Sarathi-Serve调度器是一个迭代级别的调度器，它利用chunked prefill和预填充与解码的合并来提高吞吐量，同时最小化延迟，作者称这种方法为无停顿批处理。</p><p><img src="/posts_img/Sarathi/7.png"></p><p>Sarathi-Serve首先根据用户指定的服务级别目标（SLO）计算每批可执行的最大令牌数预算。（详见下一小节）</p><p>算法流程：</p><ol><li>在每个调度迭代中，首先将所有正在decode阶段的请求放入批次中（第6-8行）</li><li>对于未完成预填充的请求，分块加入（第9-12行）</li><li>只有在所有正在运行的请求都被容纳后，才接受新请求（第13-20行）</li></ol><p>注意：在向批次添加预填充请求时，要计算在该批次的剩余token预算中可容纳的最大块大小（第11、15行）</p><p>通过限制每次迭代的计算负载，无停顿批处理确保解码不会因为并行的预填充块而经历生成停顿。</p><h2 id="确定token预算"><a href="#确定token预算" class="headerlink" title="确定token预算"></a>确定token预算</h2><p>Token预算的确定需平衡两个相互制约的因素：</p><ul><li><p>延迟目标（TBT SLO）：较小的Token预算（分块更细）可降低单次迭代延迟，但可能导致：</p><ol><li>GPU利用率下降，频繁分块增加调度开销；</li><li>KV缓存重复访问：每个分块需访问之前所有分块的KV缓存，导致显存读取次数增加（例如，分块数为N时，首个分块的KV缓存被加载N−1次）。</li></ol></li><li><p>分块预填充开销：</p><ol><li>Tile-Quantization效应：GPU矩阵乘法（Matmul）的硬件优化要求分块大小与GPU的Tile尺寸对齐（如256）。若分块大小不匹配（如257），计算时间可能增加32%。</li><li>管道并行气泡：较大的分块导致批次间运行时间差异大，产生GPU闲置（气泡）；过小的分块则因算术强度低和固定开销（如内核启动）降低效率。</li></ol></li></ul><p>优化策略：</p><ol><li>通过分析不同分块大小的性能（如预填充时间、解码延迟），找到满足TBT SLO的最大Token预算。</li><li>根据GPU的Tile尺寸调整分块大小，避免计算浪费。</li></ol><p>文中没有具体说明优化的策略，而是说使用 Vidur（LLM推理性能模拟器）进行场景化分析，结合模型、硬件和并行策略（如TP&#x2F;PP）动态优化Token预算。</p><h1 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h1><p>基于 vLLM 进行扩展和优化。通过使用 FlashAttention v2 和 FlashInfer 的内核，Sarathi-Serve 支持分块预填充。</p><h1 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h1><p>实验部分评估了Sarathi-Serve在不同模型和硬件配置下的性能，包括Mistral-7B、Yi-34B、LLaMA2-70B和Falcon-180B模型，以及单个A100 GPU、两个A100 GPU（TP2并行）、8个A40 GPU（TP4和PP2并行）等硬件配置。实验使用了两个数据集：openchat_sharegpt4和arxiv_summarization，分别代表多轮对话和科学文献摘要生成的任务。</p><p><strong>关键结论</strong></p><ol><li>吞吐量提升：Sarathi-Serve在Mistral-7B模型上实现了2.6倍的服务容量提升，在Yi-34B模型上实现了3.7倍的提升（与vLLM相比）。在Falcon-180B模型上，使用流水线并行时，Sarathi-Serve提供了高达5.6倍的端到端服务容量提升。</li><li>延迟优化：在Yi-34B模型上，与不使用分块预填充的混合批次相比，Sarathi-Serve的延迟增加仅为25%，而使用完整预填充的混合批次延迟增加了28.3倍。</li><li>流水线并行优化：Sarathi-Serve通过创建计算需求均匀的混合批次，减少了流水线并行中的气泡，从而提高了GPU利用率，使得在普通以太网连接的多节点部署中也能高效运行。</li></ol><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>另一种新兴的方法是将预填充和解码阶段在不同的副本上解耦（PD分离），如Mooncake, SplitWise, DistServe和TetriInfer 所提出的那样。</p><p><strong>优点：</strong></p><ol><li>这些解决方案完全消除了预填充和解码之间的干扰，与分块预填充相比，解耦方法可以以最大效率执行预填充（因此提供更好的TTFT）。</li></ol><p><strong>缺点：</strong></p><ol><li>解耦需要在预填充阶段完成后迁移每个请求的KV缓存，这在缺乏高带宽互连的不同副本之间具有挑战性。</li><li>导致处理prefill的GPU内存容量未被充分利用，而decoding需要存储全量的KV缓存。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
            <tag> OSDI 2024 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Llumnix-多实例LLM服务的请求动态调度</title>
      <link href="/2025/03/13/Llumnix/"/>
      <url>/2025/03/13/Llumnix/</url>
      
        <content type="html"><![CDATA[<p>Llumnix揭示了LLM服务与传统DNN推理的根本差异，提出动态调度必要性，设计了首个支持跨实例实时迁移的LLM服务系统，旨在解决大规模语言模型（LLM）推理服务中的请求高效调度问题。通过动态请求迁移和细粒度调度策略，Llumnix在降低延迟、提高优先级支持和降低成本方面表现出色。</p><p>论文：(OSDI 2024)<a href="https://www.usenix.org/system/files/osdi24-sun-biao.pdf">Llumnix: Dynamic Scheduling for Large Language Model Serving</a></p><p>代码：<a href="https://github.com/AlibabaPAI/llumnix">https://github.com/AlibabaPAI/llumnix</a></p><p>该文章参考自：</p><ol><li><a href="https://www.usenix.org/system/files/osdi24_slides-sun-biao.pdf">https://www.usenix.org/system/files/osdi24_slides-sun-biao.pdf</a></li><li><a href="https://19shuidiph.github.io/2024/12/30/paperreading-llumnix/#wechat">https://19shuidiph.github.io/2024/12/30/paperreading-llumnix/#wechat</a></li></ol><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="LLM服务特性"><a href="#LLM服务特性" class="headerlink" title="LLM服务特性"></a>LLM服务特性</h2><ul><li><p><strong>异构性</strong>：不同应用场景（如聊天、摘要、代码生成）导致请求的输入&#x2F;输出长度、延迟需求（GPT Plus）差异显著。<br>  <img src="/posts_img/Llumnix/1.png"></p></li><li><p><strong>不可预测性</strong>：生成token数量未知，GPU内存占用动态增长，传统静态调度难以应对。<br>  <img src="/posts_img/Llumnix/2.png"><br>  在推理过程中遵从<a href="https://kevin-zhang-sysu.github.io/2024/09/14/Orca/">Orca</a>中提出的选择性批处理机制，使用<a href="https://kevin-zhang-sysu.github.io/2024/09/14/vLLM/">vLLM</a>中提出的动态内存分配机制不断地为新的KVcache分配新的内存。当GPU显存满载时，会将一部分的请求（蓝色的部分）从内存中驱逐，重新放回请求队列。</p></li></ul><h1 id="存在的问题与挑战"><a href="#存在的问题与挑战" class="headerlink" title="存在的问题与挑战"></a>存在的问题与挑战</h1><ul><li><p>由于抢占带来的额外开销较大，导致P99延迟大，服务级别目标（SLO）难以满足。<br><img src="/posts_img/Llumnix/3.png"></p></li><li><p>请求之间的性能干扰。<br><img src="/posts_img/Llumnix/4.png"><br>批处理的数量越多，模型参数量越大，干扰就越明显。</p></li><li><p>内存碎片。<br>考虑到前两个挑战，应该将请求分散到不同的GPU，但这样容易造成显存的外部碎片化。导致外部请求（尤其是长请求）的延迟很高。<br><img src="/posts_img/Llumnix/5.png"></p></li><li><p>满足更高优先级<br>现在的系统一般都是平等对待所有的请求，缺乏优先级支持。（这里的平等是对每个请求的优先级都一致，关于请求公平性方面的研究可以参考FairnessLLM这篇OSDI24的工作）</p></li></ul><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>Llumnix通过运行时跨多个模型实例重新调度请求来应对上述挑战。类似于现代操作系统中的上下文切换，通过高效且可扩展的请求和内存状态实时迁移机制实现重新调度，以改善负载均衡、减少资源碎片化、区分请求优先级和SLO，还能实现弹性伸缩(更快地耗尽要终止的实例或使新实例饱和)。</p><p><img src="/posts_img/Llumnix/6.png"></p><h2 id="实时迁移"><a href="#实时迁移" class="headerlink" title="实时迁移"></a>实时迁移</h2><p><img src="/posts_img/Llumnix/7.png"></p><p>多阶段迁移：假设复制一个KVcache是0.5ms，计算一个KVcache是1ms。现在已经有了100个KVcache，一边算一边复制迁移，当源实例计算到第200个KVcache的时候，目标实例上也有199个KVcache了。然后要真正进行停等的KVcache就只有第200个这一个了，等待第200个计算完毕，复制迁移即可。</p><p>为了保证迁移的可靠性，Llumnix设计了一套handshake机制。</p><h2 id="分布式调度架构"><a href="#分布式调度架构" class="headerlink" title="分布式调度架构"></a>分布式调度架构</h2><p><img src="/posts_img/Llumnix/8.png"></p><p>Llumnix采用分布式调度架构，结合全局调度器和实例级调度器（llumlet）。全局调度器负责根据实例负载进行新请求的分发、触发跨实例迁移和控制自动扩缩容；llumlet负责本地调度、迁移协调和执行。这种架构通过分离关注点，提高了调度的可扩展性。</p><h2 id="动态调度策略"><a href="#动态调度策略" class="headerlink" title="动态调度策略"></a>动态调度策略</h2><p><img src="/posts_img/Llumnix/9.png"></p><p>引入“虚拟使用量”概念，将不同调度目标（如负载均衡、去碎片化、优先级支持）统一为简单的实例负载度量。调度策略基于虚拟使用量进行启发式的负载均衡，同时通过规则设置不同场景下的虚拟使用量，例如为高优先级请求分配更多的虚拟使用量，确保其能够平稳无干扰地运行。</p><p>如果需要空出一台机器或一个模型实例，可以将其虚拟使用量设置为最大值。这样，该实例上的任务会被调度到其他实例上。<br>调度决策基于实例的自由度（Freeness）：</p><ul><li>自由度计算公式：<br><strong>F &#x3D; (M - ∑V) &#x2F; B</strong><br>其中：<ul><li>M &#x3D; 总内存  </li><li>∑V &#x3D; 实例上所有请求的虚拟使用量之和  </li><li>B &#x3D; 批大小</li></ul></li></ul><p>调度时，优先将请求分配到自由度更高的实例上。</p><h3 id="迁移策略"><a href="#迁移策略" class="headerlink" title="迁移策略"></a>迁移策略</h3><ul><li>定期触发迁移操作，选择自由度最低的实例作为源实例，自由度最高的实例作为目标实例。</li><li>通过迁移请求实现负载均衡。</li></ul><h3 id="弹性伸缩"><a href="#弹性伸缩" class="headerlink" title="弹性伸缩"></a>弹性伸缩</h3><ul><li>如果实例的自由度极高，考虑关闭该实例以节省资源。</li><li>如果实例的自由度极低，考虑增加一个新实例以分担负载。</li></ul><h1 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h1><p>用3300行Python代码实现Llumnix，支持vLLM作为后端，利用Ray框架实现分布式协调。使用Gloo进行KVcache传输，而不是NCCL，因为后者并发调用不安全，将很多小的KVcache从GPU复制到CPU中，融合为一个大的，统一发送到目标实例。</p><h1 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h1><p>作者在16-GPU集群上使用真实（ChatGPT-4 conversation datasets, ShareGPT (GPT4) and BurstGPT (GPT4-Conversation)）和合成工作负载对Llumnix进行了评估，对比基线包括轮询调度、优化版INFaaS++等。</p><p>结果表明：Llumnix将尾延迟（P99）降低了高达15倍，将高优先级请求的延迟降低了1.5倍，并在保持类似尾延迟的情况下实现了高达36%的成本节约。</p><h1 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h1><ol><li>请求在异构实例之间的调度（不同的TP, PP等）。</li><li>实例内调度技术（如抢占式调度）优化，设计early reject避免资源的浪费等。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
            <tag> OSDI 2024 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sia-考虑集群异构性和作业弹性的DL训练系统</title>
      <link href="/2025/01/16/Sia/"/>
      <url>/2025/01/16/Sia/</url>
      
        <content type="html"><![CDATA[<p>Sia 调度器结合<strong>Gavel</strong>和<strong>Pollux</strong>这两篇文章的优势，提出了一种为<strong>异构</strong>深度学习集群的<strong>弹性</strong>资源自适应作业提供高效资源分配的方法，解决了现有调度器在异构性和资源适应性上的不足。Sia 使用<strong>Bootstrapping + 在线优化</strong>的方法，低开销、快速评估作业在不同配置下的性能，接着使用<strong>ILP算法</strong>进行资源分配，能够在大规模集群中高效扩展，并根据集群负载和作业需求动态调整。Sia 是首个支持混合并行作业弹性扩展的集群调度器。广泛的实验表明，Sia 在多个工作负载环境中显著提高了作业完成效率和资源利用率，并且具有良好的扩展性和公平性，能够支持高达 2000 GPU 的集群。</p><p>论文：(SOSP 2023)<a href="https://suhasjs.github.io/files/sia-sosp23.pdf">Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling</a></p><p>代码：<a href="https://github.com/siasosp23/artifacts">https://github.com/siasosp23/artifacts</a></p><h1 id="研究背景及内容"><a href="#研究背景及内容" class="headerlink" title="研究背景及内容"></a>研究背景及内容</h1><p>深度学习模型的训练和推理，对计算资源的要求极为庞大，对于普通企业和用户而言部署的成本巨大，基于此背景，各大企业的深度学习云计算平台应运而生。</p><p><img src="/posts_img/Sia/1.png"></p><p>用户会将深度学习模型部署到云端，进行训练或推理作业，通常会请求一定的计算资源，以保证自己对任务在时间和精度上的的需求。集群调度器决定如何将资源分配给任务，以实现公平调度、最小化任务完成时间(JCT)、提高集群利用率等目标。</p><p><img src="/posts_img/Sia/2.png"></p><p>现有的调度器在优化资源分配、减少作业完成时间 (JCT) 以及提高 GPU 利用率方面存在局限性：</p><p><img src="/posts_img/Sia/3.png"></p><p>在Sia这篇论文中，作者考虑的场景是DL任务的训练调度，的优化目标是最小化作业的JCT。过去有很多工作考虑弹性调度或者GPU资源的异构，但是没有综合两者进行考虑，其中考虑资源异构的SOTA是Gavel，考虑弹性调度的SOTA是Pollux。作者提出 Sia 调度器，结合异构性与弹性，在实际场景中优化集群资源调度。</p><h1 id="研究方法及过程"><a href="#研究方法及过程" class="headerlink" title="研究方法及过程"></a>研究方法及过程</h1><p>Sia特点：</p><ol><li><p>Sia是一个基于抢占式、轮询的调度器。</p></li><li><p>使用低开销的方法来引导（bootstrap）每个新作业的吞吐量模型，这些模型用于评估可能的资源分配。</p></li><li><p>通过引入一种新的调度公式来解决大规模搜索空间的问题，将作业及其配置与 GPU 类型和数量匹配，同时适应集群负载和作业组合的变化。</p></li></ol><p><img src="/posts_img/Sia/4.png" alt="**Sia 中作业的生命周期**"></p><p>作业提交后，它会在每种 GPU 类型上对几个批次大小进行一次profiling分析。获得资源分配后，作业开始进入一个持续优化的周期（步骤 5-8），持续进行直到它在集群中的生命周期结束。<strong>Policy</strong> 用于 ILP 问题求解，会不断优化作业的分配；<strong>Adaptive Executors</strong> 支持动态调整作业运行配置，如batch size； <strong>Goodput Estimator</strong> 提供最新的性能和梯度统计数据，以帮助决策。</p><p>作业在提交后进行快速初始配置建模，并周期性地重新分配资源以实现动态优化。</p><p>我认为，Sia这篇文章主要回答了以下三个问题。</p><h2 id="Q1-异构集群中，作业的性能因-GPU-类型和数量变化而异，如何高效建模？"><a href="#Q1-异构集群中，作业的性能因-GPU-类型和数量变化而异，如何高效建模？" class="headerlink" title="Q1. 异构集群中，作业的性能因 GPU 类型和数量变化而异，如何高效建模？"></a>Q1. 异构集群中，作业的性能因 GPU 类型和数量变化而异，如何高效建模？</h2><p>答：引入Goodput Estimator模块，使用<strong>Bootstrapping + 在线优化</strong>的方法高效建模。</p><p><img src="/posts_img/Sia/5.png"></p><ul><li>初始profiling建模时最大限度减少开销。</li><li>启发式推断未运行 GPU 类型的多 GPU 配置性能。</li><li>在线学习逐步精确吞吐量模型。</li></ul><h2 id="Q2-作业的动态弹性扩展（GPU-数量）带来巨大搜索空间，如何降低调度开销？"><a href="#Q2-作业的动态弹性扩展（GPU-数量）带来巨大搜索空间，如何降低调度开销？" class="headerlink" title="Q2. 作业的动态弹性扩展（GPU 数量）带来巨大搜索空间，如何降低调度开销？"></a>Q2. 作业的动态弹性扩展（GPU 数量）带来巨大搜索空间，如何降低调度开销？</h2><p>答：在Policy算法中构造配置集合，减小搜索空间。</p><p><img src="/posts_img/Sia/6.png"></p><p>Sia 将配置集合 𝐶 分为两部分：</p><ul><li>单节点配置（Single-node set）: 包含所有在单个节点上的资源分配。约束为 GPU 数量必须是 2 的幂，且最多不超过每节点的 GPU 数量 𝑅。如果 𝑅 不是 2 的幂，可以将节点视为多个虚拟节点。</li><li>多节点配置（Multi-node set）: 包含跨多个节点的资源分配。约束为 GPU 数量必须是每节点 GPU 数量 𝑅 的整数倍（确保使用完整节点）。</li></ul><p>Sia 利用子网形状覆盖定理（<a href="/2024/09/19/Alpa/" title="Alpa-自动生成DL&#x2F;LLM模型并行策略">Alpa-自动生成DL&#x2F;LLM模型并行策略</a>这篇文章证明的），确保所有配置的资源分配是有效的，同时避免多个作业共享节点，减少网络接口（NIC）的资源争用。</p><p>与 Pollux 的对比</p><ul><li><strong>Pollux</strong>: 在资源分配中优化整个搜索空间（即 GPU 数量 × GPU 放置组合），其复杂度为 O(N^R)，其中 N 是节点数，R 是每节点 GPU 数量。</li><li><strong>Sia</strong>: 限制配置集合大小，单节点配置数量为 log₂ R，多节点配置数量为 N，因此总体复杂度为 N + log₂ R。通过限制搜索空间，显著减少了问题复杂度，同时性能与 Pollux 相当。</li></ul><h2 id="Q3-如何设计调度算法提高集群效率（作业完成时间）？"><a href="#Q3-如何设计调度算法提高集群效率（作业完成时间）？" class="headerlink" title="Q3. 如何设计调度算法提高集群效率（作业完成时间）？"></a>Q3. 如何设计调度算法提高集群效率（作业完成时间）？</h2><p>答：<strong>有效吞吐量评估 + 整数线性规划</strong>。</p><p>Sia 使用 goodput 来衡量作业在特定配置下的效率，使得作业在不同 GPU 类型和数量上的效能具有可比性。</p><p><img src="/posts_img/Sia/7.png"></p><p>Goodput根据吞吐量和统计效率得到，用来衡量作业每秒的进度。定义详见Pollux这篇文章。</p><h3 id="Goodput-矩阵-G"><a href="#Goodput-矩阵-G" class="headerlink" title="Goodput 矩阵 G"></a>Goodput 矩阵 G</h3><p>基于最小 goodput 值归一化 Goodput 矩阵：行内值可直接比较，反映每个作业在不同配置下的效用。列间值也可比较，用于评估配置对不同作业的优先级。</p><p><img src="/posts_img/Sia/8.png"></p><p>Sia 为每个作业的每种配置计算出相应的 goodput ，并通过归一化的方式使得其既能根据给定作业选择最适合的配置，又能根据给定配置选择最适合的作业。</p><p>Sia 会一直维护这个矩阵，当新的作业到来时，会在矩阵中添加新的一行。旧的作业完成时，会删除其相应的行。使得 goodput 矩阵一直保持在最新状态，仅适用于活动中的作业。</p><h3 id="整数线性规划（ILP）"><a href="#整数线性规划（ILP）" class="headerlink" title="整数线性规划（ILP）"></a>整数线性规划（ILP）</h3><p><img src="/posts_img/Sia/9.png"></p><ul><li><strong>动态性</strong>: 矩阵 G 实时更新，随着作业的统计效率变化或模型改进不断优化分配决策。</li><li><strong>资源高效利用</strong>: 优化目标结合 goodput 和作业等待惩罚，确保资源高效分配。</li><li><strong>扩展性</strong>: 使用 ILP 求解，能够快速计算大规模集群的优化分配方案。</li></ul><p>基于这个ILP，论文中还提到了许多优化方案：</p><ul><li>重启因子 (Restart Factor)</li><li>公平性调节 (Balancing Goodput and Fairness)</li><li>混合并行训练 (Hybrid-parallel training)</li><li>抢占和预留机制 (Preemption and reservation)</li><li>其他非GPU工作负载的调度 (Other types of workloads)</li><li>…</li></ul><h1 id="实验与分析"><a href="#实验与分析" class="headerlink" title="实验与分析"></a>实验与分析</h1><h2 id="调度中的自适应表现"><a href="#调度中的自适应表现" class="headerlink" title="调度中的自适应表现"></a>调度中的自适应表现</h2><p><img src="/posts_img/Sia/10.png"></p><ul><li>Sia具有良好的自适应性，能够动态调整分配给每个作业的GPU数量和类型。</li><li>Bootstrapping确实能够快速地为作业匹配GPU类型。</li></ul><p>矩形之间的空格是Sia调度的延迟</p><h2 id="调度性能实验"><a href="#调度性能实验" class="headerlink" title="调度性能实验"></a>调度性能实验</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><h4 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h4><ul><li>异构 GPU 集群，包括 T4、V100 和 A100 等不同 GPU 类型。</li><li>集群规模从几十个到上千个 GPU 节点。</li></ul><h4 id="工作负载"><a href="#工作负载" class="headerlink" title="工作负载"></a>工作负载</h4><p>使用真实生产集群工作负载，包括：</p><ul><li><strong>Philly</strong>：微软 GPU 集群的深度学习工作负载。</li><li><strong>Helios</strong>：基于大规模高负载环境的仿真工作负载。</li><li><strong>newTrace</strong>：实际深度学习训练任务的动态工作负载。</li></ul><h4 id="对比调度器"><a href="#对比调度器" class="headerlink" title="对比调度器"></a>对比调度器</h4><ul><li><strong>Pollux</strong>：专注于作业弹性扩展的调度器。</li><li><strong>Gavel</strong>：优化异构资源分配的调度器。</li></ul><p><img src="/posts_img/Sia/11.png"></p><p><strong>平均 JCT 改善</strong></p><ul><li>在 Philly 数据集中，平均 JCT 比 Pollux 和 Gavel 分别减少 30%-93%。</li><li>在 Helios 数据集中，99 分位数 JCT 减少了 50%-80%。</li></ul><p><img src="/posts_img/Sia/12.png"></p><p><strong>GPU 小时数节省</strong></p><ul><li>在 Helios 工作负载中，GPU 小时数减少 12%-60%。</li></ul><h2 id="算法效率实验"><a href="#算法效率实验" class="headerlink" title="算法效率实验"></a>算法效率实验</h2><p><img src="/posts_img/Sia/13.png"></p><p>Sia具有良好的扩展性，Pollux的遗传算法运行速度明显较慢（比Sia的ILP公式慢100倍），Gavel要快得多，因为它没有考虑工作适应。</p><h1 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h1><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li><p><strong>联合优化异构性与作业弹性</strong></p><p>支持弹性扩展和异构 GPU 混合调度。</p></li><li><p><strong>动态吞吐量建模</strong></p><p>引入轻量级的在线学习机制，通过少量配置样本快速预测作业性能。快速适应作业需求和资源需求，有效支持异构 GPU 类型和动态负载。</p></li><li><p><strong>高效集群、扩展性与适配性</strong></p><p>支持大规模集群（上千 GPU）的高效调度，支持多种任务类型，任务并行方式、平衡公平性与效率。</p></li></ol><h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><ol><li>支持更复杂的混合并行任务调度（如流水线并行与数据并行结合）。</li><li>在超大规模集群（2000+ GPU）中进一步验证性能。</li><li>优化对其他类型工作负载（如实时推理任务）的支持。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> Train </tag>
            
            <tag> SOSP 2023 </tag>
            
            <tag> Mlsys </tag>
            
            <tag> Heterogeneity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Alpa-自动生成DL/LLM模型并行策略</title>
      <link href="/2024/09/19/Alpa/"/>
      <url>/2024/09/19/Alpa/</url>
      
        <content type="html"><![CDATA[<p>只要输入DL模型 computation graph 和 device cluster，Alpa 通过生成统一<strong>数据</strong>、<strong>运算</strong>和<strong>流水线</strong>并行性的执行计划，在<strong>可接受</strong>的时间内<strong>自动化</strong>了大型深度学习（DL）模型的模型并行训练。</p><p>现有的模型并行训练系统要么要求用户手动创建并行化计划，要么从有限的模型并行配置空间中自动生成一个。它们不足以在分布式计算设备上扩展复杂的DL模型。Alpa通过将并行性视为两个层次来分配大型DL模型的训练：**运算内并行性(inter-operator)<strong>和</strong>运算间并行性(intra-operator)**。</p><p>基于此，Alpa为大规模模型并行执行计划构建了一个新的层次空间。Alpa设计了许多编译过程，以在每个并行级别自动导出高效的并行执行计划。Alpa实现了高效的运行时，以协调分布式计算设备上的两级并行执行。评估表明，Alpa生成的并行化计划与手动调优的模型并行训练系统相匹配或优于后者，即使在它们设计的模型上也是如此。与专用系统不同，Alpa还可以推广到具有异构架构的模型和没有手动设计计划的模型。</p><p>论文：(OSDI 2022)<a href="https://www.usenix.org/system/files/osdi23-li-zhuohan.pdf">Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning</a></p><p>源代码：<a href="https://github.com/alpa-projects/alpa">https://github.com/alpa-projects/alpa</a>.</p><p>文章参考自：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/487588274">https://zhuanlan.zhihu.com/p/487588274</a></li><li><a href="https://research.google/blog/alpa-automated-model-parallel-deep-learning/">https://research.google/blog/alpa-automated-model-parallel-deep-learning/</a></li></ol><h1 id="Alpa概念"><a href="#Alpa概念" class="headerlink" title="Alpa概念"></a>Alpa概念</h1><p>作者首先将现有的机器学习并行化策略分为两类：</p><ol><li>运算内并行-Intra-Operator Parallelism：将Tensor按某些维度切裂，放到不同Device上计算的并行方式。如张量并行（如Megatron-LM）、数据并行（如Deepspeed Zero）、专家并行（如GShard MoE）。</li><li>运算间并行-Inter-Operator Parallelism：即流水线并行</li></ol><p>两类并行方式的特点：</p><ol><li>Intra-op Parallelism：通讯量较大，可充分利用带宽，切分带来的通信基本属于高效的集合通信。</li><li>Inter-op Parallelism：只在两个设备间传输数据，通讯量较小，若切点寻找的合适，则通信较小，但同步版本的策略无可避免的会引来Bubble。</li></ol><p>可以利用cluster的非对称特性，将Intra-op Parallelism映射到高带宽互联的devices上；将Inter-op Parallelism映射到低带宽互联的devices上。如此组合，就能释放更大的算力。Alpa会自动探索这些策略及组合情况。</p><p>在GPU集群中，节点内的GPU具有更高的通信带宽，可以适应运算内的并行性。然而，不同节点上的GPU通常以较低的带宽连接（例如以太网），因此首选运算间并行。</p><p>之所以要做这两种区分，目的是为了在不同的level上做策略搜索，然后将二者组合起来，生成大一统的并行方式的执行计划。总结起来就是以下两个点：</p><p>将并行度视为两级：intra-operator和inter-operator，构建分级的解空间<br>在两级空间中<strong>分别</strong>探索最优解，然后提供高效的Runtime将二者编排起来（总体上不能保证全局最优，但是实践证明在大模型上有很强的性能提升）。</p><h1 id="Alpa的Workflow"><a href="#Alpa的Workflow" class="headerlink" title="Alpa的Workflow"></a>Alpa的Workflow</h1><p>Alpa工作在DL编译层（基于XLA，在HLO上进行策略探索），所以文中也称之为——自动生成分布式策略的DL编译器。当用户给出模型计算图和设备集群时，它会进行各种操作流。</p><ol><li>运算间并行操作流-Intra-Operator Pass：传递将计算图切分为子图，将设备集群切片为子表（即分区设备集群），并确定将子图与子表的最佳配对方式。</li><li>运算内并行操作流-Inter-Operator Pass：为运算间并行的每个流水线阶段找到最佳的运算内并行计划。</li><li>运行时编排操作流-Runtime Orchestration pass：生成一个静态计划，对计算和通信进行排序，并在实际设备集群上执行分布式计算图（做runtime缝合的事情。比如stage调度，通信优化等，显然这个pass和策略搜索没什么关系）。</li></ol><p>在宏观上做DP，微观上ILP，Inter-op和Intra-op两个pass不断迭代最终获得最佳方案。</p><p>下图中，在切片子图中，红色和蓝色表示运算的划分方式，灰色表示复制的运算，绿色代表实际设备（例如GPU）：</p><p><img src="/posts_img/Alpa/1.gif" alt="Alpa概述。在切片子图中，红色和蓝色表示运算的划分方式，灰色表示复制的运算，绿色代表实际设备（例如GPU）。"></p><h2 id="运算内并行操作流（Intra-Operator-Pass）"><a href="#运算内并行操作流（Intra-Operator-Pass）" class="headerlink" title="运算内并行操作流（Intra-Operator Pass）"></a>运算内并行操作流（Intra-Operator Pass）</h2><p>与之前的研究（例如Mesh TensorFlow和GSPMD）类似，运算内并行性在设备网格上划分张量。下图显示了Transformer模型中具有给定批次、序列和隐藏维度的典型3D张量。批处理维度沿着设备网格维度0（mesh0）进行划分，隐藏维度沿着网格维度1（mesh1）进行分区，序列维度被复制到每个处理器。</p><p>在2D设备网格上划分的3D张量：<br><img src="/posts_img/Alpa/2.png" alt="在2D设备网格上划分的3D张量"></p><p>通过Alpa中张量的划分，进一步为计算图中的每个独立运算定义了一组并行化策略。在下图中展示了矩阵乘法的并行化策略示例。在运算符上定义并行化策略可能会导致张量分区上的冲突，因为一个张量既可以是一个运算符的输出，也可以是另一个运算的输入。若分区方式不匹配，两个运算之间需要重新分区，这会产生额外的通信成本。</p><p>矩阵乘法的并行化策略：<br><img src="/posts_img/Alpa/3.png" alt="矩阵乘法的并行化策略"></p><p>给定每个运算和re-partition成本，作者将运算内操作流表示为整数线性规划（ILP）问题。对于每个运算，定义一个one-hot向量来枚举分区策略。ILP的目标是最小化计算和通信成本（节点成本）和重新划分通信成本（边成本）的总和。ILP的解决方案转化为一种特定的方法来分割原始计算图。</p><p><img src="/posts_img/Alpa/4.png"></p><h2 id="运算间并行操作流（Inter-Operator-Pass）"><a href="#运算间并行操作流（Inter-Operator-Pass）" class="headerlink" title="运算间并行操作流（Inter-Operator Pass）"></a>运算间并行操作流（Inter-Operator Pass）</h2><p>运算间传递对计算图和设备集群进行切片，以实现流水线并行性。如下图所示，方框表示输入的微批，流水线阶段表示执行子图的子网格。水平维度表示时间，并显示执行微批处理的传递阶段。运算间传递的目标是最大限度地减少总执行延迟，即设备上整个工作负载执行的总和。Alpa使用动态规划（DP）算法来最小化总延迟。计算图首先被展平，然后执行Intra-Operator Pass，对设备集群到子表的所有可能分区的性能进行分析。</p><p>对于给定的时间，此图显示了分区设备集群和计算图切片（例如，阶段1、2、3）正在处理的微批（彩色框）：<br><img src="/posts_img/Alpa/6.png" alt="流水线并行。对于给定的时间，此图显示了分区设备集群和计算图切片（例如，阶段1、2、3）正在处理的微批（彩色框）。"></p><h1 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h1><p>使用8个AWS p3.16xlarge实例测试Alpa，每个实例有8个16GB V100 GPU，总共64个GPU。研究了在增加GPU数量的同时增加模型大小的弱缩放结果。我们评估了三种模型：</p><ol><li>标准Transformer模型（GPT）；</li><li>GShard MoE模型，一种混合了专家层的Transformer；</li><li>Wide ResNet，一个明显不同的模型，没有现有的专家设计的模型并行化策略。性能是通过集群上每秒实现的peta浮点运算（PFLOPS）来衡量的。</li></ol><p>作者证明，对于GPT，Alpa输出的并行化策略与现有最佳框架Megatron ML计算的策略非常相似，并与其性能相匹配。对于GShard MoE来说，Alpa在GPU（即Deepspeed）上的表现比专家设计的最佳基线高出8倍。Wide ResNet的结果表明，Alpa可以为专家尚未研究的模型生成最佳并行化策略。本文还展示了线性缩放数以供参考。</p><p><img src="/posts_img/Alpa/7.png"></p><p>在16个GPU上，Alpa将模型分为3个阶段，并分别为第1、2、3阶段分配4、4、8个GPU。在前两个阶段，数据并行性是首选，因为激活张量大于权重张量。在第三阶段，ILP求解器找到了一种划分卷积算子的非平凡方法。结果表明，对于像Wide ResNet这样的异构模型，即使对于领域专家来说，手动创建这样的策略也可能是困难的。</p><p>Alpa在16个GPU上为WideResNet找到的并行化策略：<br><img src="/posts_img/Alpa/8.png" alt="Alpa在16个GPU上为WideResNet找到的并行化策略"></p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>为分布式模型并行深度学习设计有效的并行化计划的过程历来是一项困难且劳动密集型的任务。Alpa是一个新的框架，它利用运算内和运算间的并行性进行自动化模型并行分布式训练。相信Alpa将使分布式模型并行学习规范化，并加速大型深度学习模型的开发。</p>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OSDI 2022 </tag>
            
            <tag> LLM </tag>
            
            <tag> DL </tag>
            
            <tag> Train </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vLLM-高效管理内存的LLM推理系统</title>
      <link href="/2024/09/14/vLLM/"/>
      <url>/2024/09/14/vLLM/</url>
      
        <content type="html"><![CDATA[<p>LLM在处理大量请求时面临内存使用效率低下的问题，每个请求需占用大量动态变化的内存，导致浪费并限制处理能力。本文提出了PagedAttention算法，受操作系统虚拟内存分页技术启发，开发了vLLM服务系统。vLLM通过灵活共享内存，实现几乎零内存浪费，显著降低了内存需求。测试表明，vLLM处理吞吐量比FasterTransformer和Orca提高2到4倍，且延迟保持相同，尤其在处理长文本、大模型和复杂解码时表现突出。</p><p>论文：(SOSP 2023)<img src="https://dl.acm.org/doi/pdf/10.1145/3600006.3613165" alt="Efficient Memory Management for Large Language Model Serving with PagedAttention"><br>代码：<a href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a><br>文章参考自：<a href="https://mp.weixin.qq.com/s/whsGK2gfVrIDNXTtxUUSOw">https://mp.weixin.qq.com/s/whsGK2gfVrIDNXTtxUUSOw</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>许多云服务公司正在争相提供LLM应用，但运行这些应用的成本非常高，需要大量的硬件加速器如GPU。据估计，处理一次大语言模型的请求，成本是传统关键词查询的 <strong>10</strong> 倍。由于成本如此之高，提升大语言模型的处理效率，从而降低每次请求的费用，变得越来越重要。</p><p>大语言模型（LLM）的核心是一个自回归的Transformer模型。这个模型根据输入的提示和之前生成的词（标记），一个一个地生成新的词。每次请求都需要重复这个复杂的过程，直到模型输出一个终止标记。这种逐字生成的过程需要大量内存，无法充分利用GPU的计算能力，限制了处理请求的效率。为了提高效率，可以将多个请求批量处理。但要做到这一点，需要有效管理每个请求的内存空间。</p><p><img src="/posts_img/vLLM/1.png"></p><p><strong>内存需求计算公式</strong></p><ul><li><p>模型参数</p><p>13B参数 ~ 13G 个参数 *  4个字节（FP 32） 或者 2个bytes （FP16，生产环境一般用FP16）</p></li><li><p>KV Cache</p><p>2 (key and value vectors) * hidden_size  * layer_num *  head_num * batch_size * seq_len * 参数精度（4&#x2F;2字节）</p></li></ul><p>对于更大的模型，和更长的上下文长度，KV Cache的存储需求会显著增加。由于模型的参数是固定的，而激活占用的内存很少，KV缓存的管理方式决定了最大的批处理大小。如果管理不当，KV缓存会显著限制批处理的大小，从而影响LLM的处理效率。</p><p>作者发现现有的LLM服务系统在管理KV缓存内存方面效率不高。主要因为：</p><ul><li><p><strong>内存碎片化</strong></p><p>KV Cache随着模型生成新词而 <strong>动态增长和收缩</strong>，并且它的生命周期和长度是未知的。现有系统为了在连续空间中存储请求的KV缓存，它们会 <strong>预先分配</strong> 一大块内存，按照请求的最大长度（比如2048个词）来分配，但请求的实际长度往往比最大长度短很多，且不能被其他短请求占用，导致严重的内部碎片化。而且，不同请求的预分配大小不同，还会导致 <strong>外部内存碎片化</strong> 。</p><p><img src="/posts_img/vLLM/3.png"></p><p>图中显示了两个请求：最大可能序列长度为2048的请求A和最大序列长度为512的请求B。三个主要的内存浪费来源：</p><ul><li>为未来令牌预留的插槽</li><li>过度配置最大序列长度而导致的内部碎片</li><li>来自内存分配器（如伙伴分配器）的外部碎片</li></ul><p>作者的分析结果显示，现有系统中只有20.4%-38.2%的KV缓存内存实际用于存储标记状态。</p></li><li><p><strong>无法利用内存共享</strong></p><p>LLM服务常常使用高级解码算法，比如并行采样和束搜索，这些算法会为每个请求生成多个输出。在这些场景中，请求包含的多个序列可以部分共享其KV缓存。但现有系统中KV Cache存储在不同的连续空间中，无法实现内存共享。</p></li></ul><p>为了解决上述问题，作者受 <strong>操作系统虚拟内存分页</strong> 启发，提出了PagedAttention，将请求的KV缓存分成多个小块，每个小块包含固定数量的注意力键和值。这些小块不需要存储在连续的空间中，因此可以像操作系统管理虚拟内存那样灵活地管理KV缓存。通过使用相对较小的小块并按需分配，PagedAttention缓解了内部碎片化问题。此外，由于所有小块大小相同，它还消除了外部碎片化问题。它实现了内存共享，使同一请求的不同序列甚至不同请求之间可以在小块级别共享内存。</p><p>作者基于PagedAttention构建了一个高吞吐量的分布式LLM服务引擎vLLM，该引擎实现了KV缓存内存的近乎零浪费，支持各种流行的大语言模型，如GPT、OPT和LLaMA，并且支持超过单个GPU内存容量的模型。评估结果显示，vLLM在各种模型和工作负载下，相比最先进的系统，其LLM服务吞吐量提高了2-4倍。且这种改进在处理较长序列、较大模型和更复杂的解码算法时更加显著。</p><p><img src="/posts_img/vLLM/2.png"></p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><h2 id="PageAttention算法"><a href="#PageAttention算法" class="headerlink" title="PageAttention算法"></a>PageAttention算法</h2><p>PagedAttention将每个序列的键值（KV）缓存划分为KV块。每个块包含一定数量的令牌的键和值向量。例如，键块𝐾𝑗包含的是第𝑗个块中的键向量，而值块𝑉𝑗包含的是第𝑗个块中的值向量。在注意力计算过程中，PagedAttention内核分别识别和获取不同的KV块。</p><p><img src="/posts_img/vLLM/5.png"></p><p>例如，当处理查询标记为“forth”时，内核将查询向量𝑞𝑖与每个块中的键向量𝐾𝑗相乘，以计算注意力得分𝐴𝑖𝑗。然后，它将这些得分与每个块中的值向量𝑉𝑗相乘，得出最终的注意力输出。</p><p><img src="/posts_img/vLLM/g1.png"></p><h2 id="KV-Cache管理器"><a href="#KV-Cache管理器" class="headerlink" title="KV Cache管理器"></a>KV Cache管理器</h2><p>KV Cache管理器将KV缓存组织成固定大小的KV块，类似于虚拟内存中的页面。一个请求的KV缓存被表示为一系列逻辑KV块，当新的令牌及其KV缓存生成时，从左到右填充。最后一个KV块的未填充位置保留供将来使用。</p><p>在GPU工作器上，block engine分配一块连续的GPU DRAM，并将其划分为物理KV块。KV块管理器还维护块表——每个请求的逻辑和物理KV块之间的映射关系。每个块表条目记录了逻辑块的相应物理块及其填充位置的数量。将逻辑和物理KV块分开使得vLLM可以动态增长KV缓存内存，而无需提前为所有位置预留空间，这消除了现有系统中大部分的内存浪费。</p><h2 id="基本的推理场景"><a href="#基本的推理场景" class="headerlink" title="基本的推理场景"></a>基本的推理场景</h2><p><img src="/posts_img/vLLM/6.png"></p><p>本例中，提示有7个标记，因此vLLM将前两个逻辑KV块（0和1）映射到两个物理KV块（7和1）。</p><p>在prefill步骤中，vLLM使用传统的自注意力算法生成KV缓存和第一个token。然后，vLLM将前4个标记的KV缓存存储在逻辑块0中，后续3个标记存储在逻辑块1中。剩余的位置留待后续自回归生成阶段使用。</p><p>在第1个decode步骤中，vLLM使用PagedAttention算法在物理块7和1上生成新的标记。由于最后一个逻辑块中还有一个空位，新生成的KV缓存被存储在那里，并且更新了块表中的已填充记录。</p><p>在第2个decode步骤中，由于最后一个逻辑块已满，vLLM将新生成的KV缓存存储在一个新的逻辑块中；vLLM为其分配一个新的物理块（物理块3）并将此映射存储在块表中。</p><p>对于每个decode迭代，vLLM首先选择一组候选序列进行批处理(原理同Orca选择性批处理)，并为新需求的逻辑块分配物理块。vLLM动态地为逻辑块分配新的物理块，将请求的所有内存浪费限制在一个块内，因此可以有效地利用所有内存，</p><p><img src="/posts_img/vLLM/7.png"></p><p>这两个序列的逻辑块被映射到GPU工作器中块引擎预留的不同物理块中。这两个序列的相邻逻辑块在物理GPU内存中不需要连续，物理块的空间可以被两个序列有效地利用。</p><h2 id="不同的推理场景"><a href="#不同的推理场景" class="headerlink" title="不同的推理场景"></a>不同的推理场景</h2><h3 id="Parallel-sampling（并行采样）"><a href="#Parallel-sampling（并行采样）" class="headerlink" title="Parallel sampling（并行采样）"></a>Parallel sampling（并行采样）</h3><p>在 LLM 应用中，一个输入提示可能生成多个输出，用户可以从多个候选项中选择最喜欢的结果。一个请求包含多个共享相同输入提示的样本，因此提示的KV缓存也可以共享。</p><p><img src="/posts_img/vLLM/8.png"></p><p>图中展示了两个输出的并行解码示例。为了管理共享，vLLM为每个物理块引入了引用计数，物理块7和1的引用计数都是2。在生成阶段，这两个输出生成不同的输出标记，因此需要单独的KV缓存存储。</p><p>vLLM对需要被多个序列修改的物理块实现了块级的copy-on-write机制（类似于操作系统中的写时复制技术）。当样本A1需要写入其最后一个逻辑块（逻辑块1）时，vLLM检测到相应的物理块（物理块1）的引用计数大于1，于是分配一个新的物理块（物理块3），并指示块引擎将信息从物理块1复制过去，同时将引用计数减为1。接下来，当样本A2写入物理块1时，引用计数已经减少到1，因此A2可以直接将新生成的KV缓存写入物理块1。</p><h3 id="Beam-search（束状搜索）"><a href="#Beam-search（束状搜索）" class="headerlink" title="Beam search（束状搜索）"></a>Beam search（束状搜索）</h3><p>在LLM（大型语言模型）任务如机器翻译中，用户通常期望得到最合适的前𝑘个翻译结果。Beam search是一种常用的解码算法，用于从LLM中生成最可能的输出序列。算法依赖于一个参数𝑘，该参数决定每一步保留的最顶尖的候选序列数量。在解码过程中，Beam search会扩展每个候选序列，考虑所有可能的标记，计算它们各自的概率，并从𝑘 · |𝑉|个候选中保留前𝑘个最可能的序列，其中|𝑉|是词汇表的大小。</p><p><img src="/posts_img/vLLM/9.png"></p><p>与并行解码不同，Beam search不仅可以共享初始提示块，还可以在不同的候选序列之间共享其他块，并且这些共享模式会随着解码过程的推进动态变化，类似于操作系统中由复合分叉创建的进程树。</p><p>图中展示了vLLM如何管理一个𝑘&#x3D;4的Beam search示例中的KV块。在图中虚线之前，每个候选序列已经使用了4个完整的逻辑块。所有Beam候选共享第一个块0（即提示）。候选3从第二块开始与其他候选分开。候选0-2共享前三个块，并在第四块分开。在随后的迭代中，最可能的前4个候选都源自候选1和2。由于原始候选0和3不再是顶尖候选，它们的逻辑块被释放，对应的物理块引用计数减少。vLLM释放所有引用计数为0的物理块（块2、4、5、8）。然后，vLLM分配新的物理块（块9-12）以存储新候选生成的KV缓存。</p><p>现在，所有候选共享块0、1、3；候选0和1共享块6，候选2和3进一步共享块7。以前的LLM服务系统需要在Beam候选之间频繁复制KV缓存，这会带来很大的内存复制开销。例如，在图所示的情况下，虚线之后，候选3需要复制候选2的大部分KV缓存以继续生成。vLLM通过物理块共享显著减少了这种频繁的内存复制开销。在vLLM中，不同Beam候选的大多数块可以共享。只有当新生成的标记位于旧的共享块内时，才会应用写时复制机制，这仅涉及复制一个块的数据。</p><p>总结来说，vLLM通过有效的物理块共享和写时复制机制，大大提高了Beam search解码过程中的内存利用效率，减少了内存复制开销。</p><h3 id="Shared-prefix（共享前缀）"><a href="#Shared-prefix（共享前缀）" class="headerlink" title="Shared prefix（共享前缀）"></a>Shared prefix（共享前缀）</h3><p>许多用户提示可能共享一个前缀，因此LLM服务提供者可以提前存储这个前缀的KV缓存，以减少在前缀上的重复计算。在vLLM中，可以通过为一组预定义的共享前缀保留一组物理块来方便地实现这一点，就像操作系统处理跨进程共享库一样。</p><p><img src="/posts_img/vLLM/10.png"></p><h3 id="混合解码"><a href="#混合解码" class="headerlink" title="混合解码"></a>混合解码</h3><p>基于上面的内存管理方法，vLLM能够同时处理具有不同甚至是混合解码偏好的请求，这是现有系统无法高效完成的。这是因为vLLM通过一个公共映射层隐藏了不同序列之间复杂的内存共享，该映射层将逻辑块转换为物理块。</p><p>LLM及其执行内核只需看到每个序列的一组物理块ID，而不需要处理跨序列的共享模式。与现有系统相比，这种方法拓宽了具有不同采样需求的请求的批处理机会，从而最终提高了系统的整体吞吐量。</p><h2 id="调度与抢占"><a href="#调度与抢占" class="headerlink" title="调度与抢占"></a>调度与抢占</h2><p>当请求流量超过系统容量时，采用先到先服务（FCFS）调度策略，确保公平性并防止饥饿。</p><ol><li><p>驱逐<br>通常，驱逐策略使用启发式方法来预测哪个块将来访问的最远，并驱逐该块。由于一个序列的所有块都是一起访问的，所以我们实施了一种全有或全无的驱逐策略，即要么驱逐序列的所有块，要么一个也不驱逐。</p></li><li><p>恢复<br><strong>交换</strong>：将被驱逐的块复制到 CPU 内存中。当 vLLM 为新令牌耗尽自由物理块时，它选择一组序列进行驱逐，并将它们的 KV 缓存传输到 CPU。一旦抢占了一个序列并驱逐了它的块，vLLM 就停止接受新请求，直到所有抢占的序列完成为止。一旦一个请求完成，其块就会从内存中释放，抢占的序列的块就会被重新引入以继续处理该序列。<br><strong>重新计算</strong>：重新计算的延迟可能低于交换延迟，性能取决于 CPU RAM 和 GPU 内存之间的带宽以及 GPU 的计算能力。为了了解两种方法之间的权衡，作者评估了它们的端到端性能，并对它们的开销进行微基准测试。结果显示，交换在小块较小时产生了过多的开销。这是因为小块通常导致CPU和GPU之间大量的小数据传输，从而限制了有效的PCIe带宽。</p></li></ol><p>相比之下，重新计算的开销在不同的块大小下保持不变，因为重新计算不利用KV块。因此，当块大小较小时，重新计算更有效率，而当块大小较大时，交换更有效率，尽管重新计算的开销从未高于交换的延迟的20%。对于从16到64的中等块大小，这两种方法表现出相当的端到端性能。</p><p><img src="/posts_img/vLLM/11.png"></p><h2 id="分布式执行"><a href="#分布式执行" class="headerlink" title="分布式执行"></a>分布式执行</h2><p><img src="/posts_img/vLLM/4.png"></p><p>vLLM使用的Megatron-LM张量并行策略。每个模型分片处理相同的输入，因此需要相同位置的KV缓存。因此，vLLM在集中式调度器中具有一个单一的KV缓存管理器。不同的GPU工作节点共享该管理器，以及逻辑块到物理块的映射。</p><h1 id="系统实现"><a href="#系统实现" class="headerlink" title="系统实现"></a>系统实现</h1><p>vLLM基于FastAPI的前端和基于GPU的推理引擎。前端扩展了OpenAI API接口。</p><p>vLLM引擎由8.5K行Python代码和2K行C++&#x2F;CUDA代码编写而成。使用Python开发了调度器和块管理器等控制相关组件，同时为PagedAttention等关键操作开发了自定义CUDA核心。在分布式GPU工作节点之间的张量通信中，使用NCCL。</p><h2 id="核心级优化"><a href="#核心级优化" class="headerlink" title="核心级优化"></a>核心级优化</h2><p>由于PagedAttention引入了现有系统不高效支持的内存访问模式，开发了几个GPU Kernel进行优化。</p><ul><li>融合重塑和块写入：在每个Transformer层中，将新的KV缓存分割成块，重塑为针对块读取进行优化的内存布局，然后保存在由块表指定的位置。为了最小化核心启动开销，将它们融合成单个核心。</li><li>融合块读取和注意力：调整了FasterTransformer中的注意力核心，根据块表读取KV缓存并实时执行注意力操作。为了确保合并内存访问，为每个块分配了一个GPU线程束。此外，增加了对请求批处理中可变序列长度的支持。</li><li>融合块复制：通过写时复制机制发出的块复制操作可能在不连续的块上操作。如果使用cudaMemcpyAsync API，这可能导致大量小数据移动的调用。为了减轻开销，实现了将不同块的复制操作批处理到单个核心启动中。</li></ul><h1 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h1><p>虚拟内存和分页技术在其他GPU工作负载中的应用：</p><p>虚拟内存和分页在LLM（大型语言模型）推理中的有效性源于其动态内存分配需求（因为输出长度不可预知）及其对GPU内存容量的依赖。然而，这并不适用于所有GPU工作负载。例如，在DNN（深度神经网络）训练中，张量形状通常是静态的，因此可以预先优化内存分配。在非LLM的DNN推理中，内存效率的提升可能不会带来性能提升，因为性能主要受计算能力限制。在这些场景中，vLLM技术可能反而会因为内存重定向和非连续内存块带来的额外开销而降低性能。</p><p>然而，对于具有与LLM推理类似特性的其他工作负载，vLLM技术的应用仍具有潜力。</p><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><h2 id="通用模型服务系统"><a href="#通用模型服务系统" class="headerlink" title="通用模型服务系统"></a>通用模型服务系统</h2><p>Clipper[11]、TensorFlow Serving[33]、Nexus[45]、InferLine[10]和Clockwork[20]是一些早期的通用模型服务系统。他们研究为单个或多个模型提供服务的批处理、缓存、放置和调度。最近，DVABatch[12]引入了多入口多出口批处理。REEF[21]和Shepherd[61]建议优先抢占。AlpaServe[28]利用模型并行性进行统计复用。然而，这些通用系统未能考虑LLM推理的自回归特性和令牌状态，导致错过了优化机会。</p><h2 id="Transformer专用推理服务系统"><a href="#Transformer专用推理服务系统" class="headerlink" title="Transformer专用推理服务系统"></a>Transformer专用推理服务系统</h2><p>这些系统利用GPU内核优化[1,29,31,56]、高级批处理机制[14,60]、模型并行[1,41,60]和参数共享[64]来实现高效服务。其中，Orca[60]与本文的方法最相关。与Orca相比。Orca[60]中的迭代级调度和vLLM中的PagedAttention是互补的技术：虽然这两个系统都旨在提高GPU利用率，从而提高LLM服务的吞吐量，但Orca是通过调度和交织请求来实现的，这样可以并行处理更多请求，而vLLM是通过提高内存利用率来实现的。通过减少内存碎片和启用共享，vLLM并行批处理更多请求，与Orca相比，速度提高了2-4倍。事实上，Orca中请求的细粒度调度和交织使内存管理更具挑战性，使vLLM中提出的技术变得更加关键。</p><h2 id="内存优化"><a href="#内存优化" class="headerlink" title="内存优化"></a>内存优化</h2><p>GPU的计算能力和内存容量之间的差距越来越大，导致内存成为训练和推理的瓶颈。交换[23,42,55]、重新计算[7,24]及其组合[40]已被用于减少训练的峰值内存。值得注意的是，FlexGen[46]研究了在GPU内存有限情况下如何通过交换LLM推理的权重和令牌状态，但它不针对在线服务设置。OLLA[48]优化了张量的生存期和位置以减少碎片化，但它不进行细粒度块级管理或在线服务。FlashAttention[13]应用平铺和内核优化来减少注意力计算的峰值内存并降低I&#x2F;O成本。本文介绍了一种在线服务环境下块级内存管理的新思路。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文提出了一种新的注意力算法PagedAttention，该算法允许在非连续的分页内存中存储注意力键和值，并介绍了vLLM，一种通过PagedAttention实现高效内存管理的高吞吐量大语言模型（LLM）服务系统。受操作系统的启发，展示了如何改编诸如虚拟内存和写时复制等成熟技术，以高效管理KV缓存并处理LLM服务中的各种解码算法。实验结果表明，vLLM相比现有的最先进系统实现了2-4倍的吞吐量提升。</p>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
            <tag> SOSP 2023 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Orca-大模型推理系统开山之作</title>
      <link href="/2024/09/14/Orca/"/>
      <url>/2024/09/14/Orca/</url>
      
        <content type="html"><![CDATA[<p>本文提出了一种新的分布式服务系统 ORCA，针对大规模 Transformer 模型的自回归生成任务，解决了现有推理服务系统在多迭代特性任务上表现不佳的问题。ORCA 通过引入 <strong>迭代级调度</strong> 和 <strong>选择性批处理</strong> 两项技术，实现了更灵活高效的调度。实验结果表明，在处理 GPT-3 175B 模型时，ORCA 在保持相同延迟的情况下，吞吐量较 NVIDIA FasterTransformer 提升了 36.9 倍。</p><p>论文：(OSDI 2022)<a href="https://www.usenix.org/conference/osdi22/presentation/yu">ORCA: A Distributed Serving System for Transformer-Based Generative Models</a></p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>本文讨论了在服务大规模 Transformer 模型时面临的挑战，特别是用于生成任务的模型，如语言生成、代码生成等。典型的例子包括 GPT-3 这样的模型。随着对模型需求的不断增长，低延迟和高吞吐量成为推理系统的目标，早期通过使用如Triton和FasterTransformer的组合来部署服务，Triton主要负责将多个客户端请求分组到一个批中，而FasterTransformer作为模型推理进行优化的执行引擎从Triton接收该批，并以批处理的方式进行推理过程。这些模型通过一种多次迭代的自回归方式处理文本（即每次生成一个词元），这给现有系统带来了特定的挑战。</p><!-- <p align="center">  <img src="posts_img/Orca/1.png" alt="Description of image" width="700" /></p> --><p><img src="/posts_img/Orca/1.png?50"></p><h1 id="问题陈述"><a href="#问题陈述" class="headerlink" title="问题陈述"></a>问题陈述</h1><p>现有的服务基于 Transformer 的生成模型的系统存在以下几个关键低效之处：</p><ul><li>批调度：一个批次中已完成的请求必须等待整个批次完成，导致延迟增加。</li><li>排队延迟：新请求在当前批次完成之前无法处理。</li><li>多次迭代开销：由于每个序列中的词元必须通过迭代处理，同一模型需要多次调用来完成单个推理请求，这影响了吞吐量。</li></ul><p><img src="/posts_img/Orca/2.png?50"></p><p>注意，在语言模型的训练中不会出现提前完成和延迟加入请求的问题；训练过程通过使用teacher forcing technique在一次迭代中完成对整个批次的处理。</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>作者提出了 ORCA，一个优化大规模 Transformer 模型的分布式服务系统。ORCA 引入了两个关键技术：</p><h2 id="迭代级调度"><a href="#迭代级调度" class="headerlink" title="迭代级调度"></a>迭代级调度</h2><p>与其对整个请求进行批处理，ORCA 对每次迭代进行调度：</p><p><img src="/posts_img/Orca/3.png?50"></p><ul><li>更快返回已完成的请求，减少延迟。</li><li>在当前迭代后立即处理新到的请求，允许更快的响应。</li></ul><h2 id="选择性批处理"><a href="#选择性批处理" class="headerlink" title="选择性批处理"></a>选择性批处理</h2><p>针对上述所提到的迭代级调度的方法，在一个批中的请求有以下情况：(1) 所处阶段不同，prefill &#x2F; decode。（2）序列长度不同。调度器希望同时存在这两个符合批处理条件的请求才能执行批处理，随着批处理大小的增加，这种可能性进一步呈指数级下降，因此使用大批处理大小来提高吞吐量而不影响延迟是困难的。</p><p><img src="/posts_img/Orca/4.png?100x"></p><p>针对此问题，作者提出的选择性批处理方法仅将批处理应用于特定操作（例如矩阵乘法），而对于无法批处理的操作（如注意力机制，输入张量大小可变的操作）则单独处理，这种选择性方法提升了效率，而不会增加额外的计算成本。</p><h1 id="调度策略"><a href="#调度策略" class="headerlink" title="调度策略"></a>调度策略</h1><p>本文中调度算法考虑的因素：</p><ul><li>FCFS</li><li>在内存限制的情况下，尽可能地增大batch_size以增大吞吐量</li></ul><p><img src="/posts_img/Orca/7.png?50"></p><p>通过流水线并行，迭代级调度可以消除bubble的影响</p><p><img src="/posts_img/Orca/8.png?50"></p><h1 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h1><p>ORCA 使用了 <strong>层内并行</strong> 和 <strong>层间并行</strong> 模型来将大规模模型分布在多个 GPU 上。通过采用 GPU 分区，该系统可以扩展到拥有数百亿参数的 Transformer 模型（如 GPT-3 175B 及更大）。</p><table><thead><tr><th align="center"><img src="/posts_img/Orca/5.png" alt="Image 1"></th><th align="center"><img src="/posts_img/Orca/6.png" alt="Image 2"></th></tr></thead></table><p>该架构主要包括：</p><ul><li><strong>调度器</strong>：根据调度算法选择批。</li><li><strong>引擎主控</strong>：管理迭代级调度，并将任务分发到 GPU。</li><li><strong>工作单元</strong>：每个工作单元负责模型的一部分，可以在多个 GPU 之间处理请求。</li><li><strong>注意力 K&#x2F;V 管理器</strong>：管理和维护跨迭代的注意力键值对，允许其他操作进行选择性批处理。</li></ul><h1 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h1><h2 id="基准测试"><a href="#基准测试" class="headerlink" title="基准测试"></a>基准测试</h2><p>ORCA 与 NVIDIA 的 FasterTransformer 进行了对比测试，测试的模型规模达到了 3410 亿参数（如 GPT-3）。实验结果显示：</p><ul><li>吞吐量提升 <strong>36.9</strong> 倍，并且延迟保持相同。</li><li>由于迭代级调度，<strong>排队时间</strong> 显著减少，从而加快响应速度。</li><li>在不同的批处理规模和工作负载下，ORCA 显著优于传统系统。</li></ul><p><img src="/posts_img/Orca/9.png?50"></p><h1 id="先前工作和讨论"><a href="#先前工作和讨论" class="headerlink" title="先前工作和讨论"></a>先前工作和讨论</h1><h2 id="BatchMaker-系统"><a href="#BatchMaker-系统" class="headerlink" title="BatchMaker 系统"></a>BatchMaker 系统</h2><p>BatchMaker 是一个用于 RNN（循环神经网络）的服务系统，能够以 RNN 单元的粒度进行调度和批处理。由于 RNN 每个单元执行相同的计算，因此可以将相同的单元进行批处理，即使它们处于不同的位置（即不同的词元索引）。这种灵活性允许新到达的请求加入当前正在执行的批次，而不需要等待整个批次完成。</p><p>然而，BatchMaker 无法对 Transformer 模型进行同样的批处理，因为 Transformer 中每个单元（如注意力机制中的键值对）随词元索引变化而不同。因此，BatchMaker 在处理不同词元时无法实现批处理，导致处理大多是串行的，并且无法支持大规模模型的并行训练。</p><h2 id="ORCA-的改进"><a href="#ORCA-的改进" class="headerlink" title="ORCA 的改进"></a>ORCA 的改进</h2><p>相比于 BatchMaker，ORCA 的设计基于最大化每次模型参数读取时的计算效率。这是因为对于大模型来说，从 GPU 全局内存读取参数是影响整体执行时间的主要瓶颈。ORCA 通过迭代级调度和选择性批处理来处理所有已准备好的词元，无论它们是否可以被批处理（如 Attention 操作不能被批处理）。</p><h2 id="针对-Transformer-模型的专用推理引擎"><a href="#针对-Transformer-模型的专用推理引擎" class="headerlink" title="针对 Transformer 模型的专用推理引擎"></a>针对 Transformer 模型的专用推理引擎</h2><p>Transformer 模型的性能促使了专门的推理系统的发展，如 FasterTransformer、LightSeq、TurboTransformers 等。这些系统主要作为现有服务系统（如 Triton 或 TensorFlow Serving）的后端推理引擎，仍采用传统的请求级调度。相比之下，ORCA 通过引入更细粒度的调度机制来提升效率。</p><h2 id="服务系统与推理引擎的接口"><a href="#服务系统与推理引擎的接口" class="headerlink" title="服务系统与推理引擎的接口"></a>服务系统与推理引擎的接口</h2><p>当前的通用服务系统如 Triton 和 Clipper 提供了抽象层来处理客户端请求，并调度底层的推理引擎。虽然这种设计分离了服务层和执行层的实现，但在处理像 GPT 这种多迭代特性的模型时存在局限性。ORCA 通过紧密集成调度器与引擎，简化了迭代级调度和选择性批处理的应用。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>ORCA 在处理 Transformer 模型的多次迭代和自回归特性方面显示出显著的改进。<strong>迭代级调度</strong> 和 <strong>选择性批处理</strong> 策略使 ORCA 能够高效处理大规模生成模型，提升了系统的性能和响应速度。</p>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OSDI 2022 </tag>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MOE基础介绍</title>
      <link href="/2024/09/13/MOE-Intro/"/>
      <url>/2024/09/13/MOE-Intro/</url>
      
        <content type="html"><![CDATA[<p>MOE重要性：坊间一直流传GPT-4是MoE模型<br>本文主要参考自：<a href="https://huggingface.co/blog/zh/moe">https://huggingface.co/blog/zh/moe</a></p><h1 id="什么是MOE"><a href="#什么是MOE" class="headerlink" title="什么是MOE"></a>什么是MOE</h1><p>基于 Transformer 架构的模型，混合专家模型主要由两个关键部分组成:</p><p>● <strong>稀疏 MoE 层</strong>: 这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构，甚至可以是 MoE 层本身，从而形成层级式的 MoE 结构。</p><p>● <strong>门控网络或路由</strong>: 这个部分用于决定哪些令牌 (token) 被发送到哪个专家。例如，在下图中，“More”这个令牌可能被发送到第二个专家，而“Parameters”这个令牌被发送到第一个专家。有时，一个令牌甚至可以被发送到多个专家。令牌的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。</p><p><img src="/posts_img/MOE-Intro/1.png"></p><div style="text-align: center;">  <a href="https://arxiv.org/pdf/1701.06538">Outrageously Large Neural Network 论文中的 MoE layer</a></div><p><img src="/posts_img/MOE-Intro/2.png"></p><div style="text-align: center;">  <a href="https://arxiv.org/pdf/2101.03961">Switch Transformers paper 论文中的 MoE layer</a></div><h1 id="混合专家模型（MoEs）简短总结"><a href="#混合专家模型（MoEs）简短总结" class="headerlink" title="混合专家模型（MoEs）简短总结"></a>混合专家模型（MoEs）简短总结</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><p>● 与稠密模型相比，<strong>预训练速度更快</strong><br>● 与具有相同参数数量的模型相比，具有更快的 <strong>推理速度</strong><br>● 与具有相同激活参数的稠密模型模型相比，具有更高的 <strong>推理精度</strong></p><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>● 需要 <strong>大量显存</strong>，因为所有专家系统都需要加载到内存中（现有offload技术）<br>● 在 <strong>微调方面</strong> 存在诸多挑战，但 <a href="https://arxiv.org/pdf/2305.14705">近期的研究</a> 表明，对混合专家模型进行指令调优具有很大的潜力<br>● 不同的专家倾向于专注于不同的 <strong>语义</strong>，而不是特定 <strong>领域</strong><br>● 由于设备间需要传递数据，网络带宽常常成为性能瓶颈，应采取适当的并行化策略（3D并行+专家并行）</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017)<br>[2] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Jan 2022)</p>]]></content>
      
      
      <categories>
          
          <category> Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> MOE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MOE利用Offload进行高效推理</title>
      <link href="/2024/09/12/MOE-Offloading/"/>
      <url>/2024/09/12/MOE-Offloading/</url>
      
        <content type="html"><![CDATA[<p>这篇文章提出了如何在资源受限的消费级硬件上高效地运行稀疏专家混合（MoE）语言模型的方法。将Mixtral-8x7B这个需要100G以上算力才能部署的模型在12G显存+11G内存的组合下跑出来。<br>论文：<a href="https://arxiv.org/abs/2312.17238">https://arxiv.org/abs/2312.17238</a><br>Colab代码：<a href="https://colab.research.google.com/drive/1ZkC0k487oBEF19R8_9nq2MSHFyQ6OspG?usp=drive_link">https://colab.research.google.com/drive/1ZkC0k487oBEF19R8_9nq2MSHFyQ6OspG?usp=drive_link</a></p><h1 id="引言与背景"><a href="#引言与背景" class="headerlink" title="引言与背景"></a>引言与背景</h1><p>论文的引言部分介绍了大规模预训练语言模型（LLMs）在自然语言处理领域的重要性。这些模型如GPT-3、GPT-4以及其他开放访问的LLMs（如LLaMA、Falcon、BLOOM等）推动了语言技术的迅猛发展。然而，LLMs 的庞大参数量使得它们的推理成本极高，通常需要高端的GPU设备才能运行，限制了它们在普通硬件上的使用。<br>为了缓解这个问题，稀疏的专家混合（MoE）模型被提出。MoE通过只激活模型中的一部分“专家”来计算每个输入，从而提高了计算效率。然而，MoE模型的规模依然庞大，尤其是在需要多GPU的环境下。因此，如何在消费级硬件上运行这些模型是一个重要的研究问题。</p><h1 id="三大解决策略"><a href="#三大解决策略" class="headerlink" title="三大解决策略"></a>三大解决策略</h1><p>Mixtral-8x7B模型中的总参数为46.7亿，专家构成45.1亿（96.6%），在内存受限的情况下，减少专家切换时GPU与RAM之间的数据传输对MoE模型进行推理很关键。作者主要提出通过LRU缓存（LRU caching）和专家的推测性提前加载（Speculative Expert Loading）来减少GPU与RAM之间的数据传输，从而加速推理过程。关键创新点包括：</p><ol><li><p>LRU缓存专家重用模式：MoE模型在处理连续的token时，某些专家会被频繁地重用。因此，作者设计了一种LRU缓存机制，利用这种专家重用的规律来减少GPU-RAM之间的通信开销。<br><img src="/posts_img/MOE-Offloading/1.png"></p></li><li><p>推测性专家加载：由于推理过程中无法提前确定下一层需要加载的专家，因此作者提出了一种基于推测的加载机制，通过对前一层的隐藏状态应用下一层的门控函数来猜测即将需要的专家（可能是因为有residual的原因）。这种机制在推测正确时，下一层的计算可以立即开始，显著减少了推理延迟。</p></li><li><p>混合量化技术：在模型压缩方面，作者使用了一种半二次量化（Half Quadratic Quantization, HQQ）的方法对专家层进行更高的压缩，同时保持其他层的较高精度。这种量化策略有效减少了模型大小，并保持了较好的推理性能。</p></li></ol><h1 id="实验与评估"><a href="#实验与评估" class="headerlink" title="实验与评估"></a>实验与评估</h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>用到的模型：Mixtral 8x7B ，一个主流的MOE模型，在大多数基准测试中优于或等价于Llama2 70B, GPT3.5，且推理速度比Llama2 70B快六倍！<br>Mixtral 8x7B 是decoder-only model, 其中 FFN 从8个不同的参数组（专家）中进行挑选，在每一层，每个token, router network 都会选2个组来进行生成并进行组合：</p><ol><li>支持32K上下文</li><li>支持英语，法语，意大利语，德语，西班牙语（中文支持很差）</li><li>在代码生成上很强</li><li>能被微调成一个高分（MT-Bench）的 instruction-following model</li></ol><h2 id="评估结论"><a href="#评估结论" class="headerlink" title="评估结论"></a>评估结论</h2><p>论文在不同硬件配置（如RTX 3060、T4等）下对提出的方法进行了详尽的实验评估，得出了以下几个主要结论：</p><ol><li><p>专家缓存与推测加载的有效性：通过测试不同缓存大小和提前加载的专家数量，作者发现缓存命中率和推测加载的准确率显著提高了模型的推理速度。例如，在缓存大小为4时，缓存命中率可以达到约0.8，推测加载大小为2时，推测加载的准确率则可达到0.9以上。<br><img src="/posts_img/MOE-Offloading/2.png"></p></li><li><p>量化对模型性能的影响：通过对模型进行不同量化比特的测试，作者验证了在保持较好准确率的同时，量化可以有效减少模型大小。例如，使用2-bit量化时，模型的推理延迟显著降低，同时在WikiText2和C4数据集上的困惑度仅略有上升。</p></li><li><p>实际推理性能：在使用完整的算法时，消费级硬件上如RTX 3060和T4可以达到每秒生成2-3个token的性能，远远优于直接在设备内存不足的情况下推理时的性能表现。<br><img src="/posts_img/MOE-Offloading/3.png"></p></li></ol><h1 id="结论与未来工作"><a href="#结论与未来工作" class="headerlink" title="结论与未来工作"></a>结论与未来工作</h1><p>论文总结了该方法在推理速度上相较于传统的加载方式有显著提高，尤其是在消费级硬件和免费云平台（如Google Colab）上，使得大规模稀疏MoE模型的使用更加广泛化。未来的研究方向可能包括进一步优化专家预测加载算法，探索其他的推理加速方法。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这篇论文解决了大规模稀疏专家模型在推理时的硬件瓶颈问题，提出了一种通过专家缓存与预测加载来优化推理速度的方案，并使用混合量化技术在保证准确率的同时大幅减少了模型大小和推理时间。对于希望在低端硬件上使用大规模语言模型的研究人员来说，本文的贡献提供了一个具有实用价值的解决方案。</p><h1 id="Ideas"><a href="#Ideas" class="headerlink" title="Ideas"></a>Ideas</h1><ol><li>“Note that out of 46.7B total parameters in the Mixtral-8x7B model, the experts constitute 45.1B (96.6%).” 专家参数占了主导位置，这种异构型能否用于边缘-云端计算，隐私保护等</li><li>对于多用户多对话的在线推理服务系统，采取批量、并行的策略合理使用experts参数，增加吞吐、降低延迟等</li><li>将这种offload方法引入到训练过程中，可以显著扩大模型或数据集的规模</li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
            <tag> MOE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/09/12/hello-world/"/>
      <url>/2024/09/12/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><p>博客搭建参考文章：<a href="https://blog.csdn.net/mjh1667002013/article/details/129290903">【Hexo】Hexo搭建Butterfly主题并快速美化_hexo butterfly-CSDN博客</a></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
