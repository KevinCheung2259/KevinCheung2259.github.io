---
layout: fast
title: EP02-vLLMæºç è®²è§£ç›´æ’­ç¬”è®°-åˆ†å¸ƒå¼é€šä¿¡ä¸å¹¶è¡Œç­–ç•¥
date: 2025-03-25 17:32:10
summary: 
cover: posts_img/vLLM/cover.png
categories: 
  - Note
tags: 
  - vLLM
  - LLM
  - Inference
---

# [FIXME][EP02] vLLM æºç è®²è§£ç›´æ’­ç¬”è®°

## EP02: åˆ†å¸ƒå¼é€šä¿¡ä¸å¹¶è¡Œç­–ç•¥

ç›´æ’­å›çœ‹é“¾æ¥ï¼šhttps://www.youtube.com/watch?v=W83Zgbg8SkE&t=4s

ç‰¹åˆ«é¸£è°¢ï¼šæœˆçƒå¤§å”ï¼ŒDu Kuntaiï¼ŒCheng Yihua å¤§ä½¬å¸¦æ¥çš„ç²¾å½©è®²è§£

### ğŸ“Œ 1. GroupCoordinator ç±»è§£æ
'vllm/distributed/parallel_state.py'
```python
# å¯ä»¥æŠŠGroupCoordinatoræƒ³è±¡æˆä¸€ä¸ªç¾¤èŠ
class GroupCoordinator:
    """
    # PyTorch ProcessGroup çš„å°è£…ï¼Œç®¡ç†è¿›ç¨‹ç»„é—´çš„é€šä¿¡

    PyTorch ProcessGroup wrapper for a group of processes.
    PyTorch ProcessGroup is bound to one specific communication backend,
        e.g. NCCL, Gloo, MPI, etc.
    GroupCoordinator takes charge of all the communication operations among
        the processes in the group. It can route the communication to
        a specific implementation (e.g. switch allreduce implementation
        based on the tensor size and cuda graph mode).
    """

    # available attributes:
    
    # è¿™ä¸ªç¾¤é‡Œè¡¨ç¤ºçš„æ˜¯æˆ‘æ˜¯è°
    rank: int  # global rank

    # åœ¨è¿™ä¸ªç¾¤é‡ŒåŠ ä¸Šæˆ‘è¿˜æœ‰å“ªäº›äºº
    ranks: List[int]  # global ranks in the group

    # åœ¨ç¾¤é‡Œçš„äººæ•°
    world_size: int  # size of the group

    # difference between `local_rank` and `rank_in_group`:
    # if we have a group of size 4 across two nodes:
    # Process | Node | Rank | Local Rank | Rank in Group
    #   0     |   0  |  0   |     0      |       0
    #   1     |   0  |  1   |     1      |       1
    #   2     |   1  |  2   |     0      |       2
    #   3     |   1  |  3   |     1      |       3

    # å¯¹åº”é€»è¾‘ä¸Šçš„GPU idï¼ˆæ¯”å¦‚ä¸€å°8å¡æœºä¸Šï¼Œ0å·GPUè¢«å ç”¨äº†çš„è¯ï¼ŒSET_CUDA_DEVICE=1-6ä¹‹åï¼Œå†…éƒ¨æ˜ å°„ä¸º0-5ï¼‰
    local_rank: int  # local rank used to assign devices

    rank_in_group: int  # rank inside the group

    # CPUçš„é€šä¿¡æ›´åŠ å¯æ§ï¼Œæ›´å¥½åšï¼Œç”¨äºä¸»æœºç«¯åŒæ­¥ï¼ˆå¦‚åˆå§‹åŒ–é˜¶æ®µï¼‰ï¼Œæ”¯æŒä»»æ„é€šä¿¡åç«¯ï¼ˆGloo/MPIï¼‰
    cpu_group: ProcessGroup  # group for CPU communication

    # ç”¨äºè®¾å¤‡é—´æ•°æ®ä¼ è¾“ï¼ˆå¿…é¡»ä½¿ç”¨æ”¯æŒGPUçš„åç«¯ï¼Œå¦‚NCCLï¼‰
    device_group: ProcessGroup  # group for device communication

    use_pynccl: bool  # a hint of whether to use PyNccl
    use_custom_allreduce: bool  # a hint of whether to use CustomAllreduce
    # communicators are only created for world size > 1
    pynccl_comm: Optional[Any]  # PyNccl communicator
    ca_comm: Optional[Any]  # Custom allreduce communicator
    mq_broadcaster: Optional[Any]  # shared memory broadcaster
```

### âš¡ 2. å¹¶è¡Œç­–ç•¥è¯¦è§£

- TPï¼ˆå¼ é‡å¹¶è¡Œï¼‰
    - éœ€è¦allreduceï¼Œé€šä¿¡é‡å¤§ï¼Œå¯¹äºé€šä¿¡éœ€æ±‚è¾ƒé«˜
    - Infraç«¯
        - é€šä¿¡è®¾å¤‡
            - NVLink: GPUä¹‹é—´çš„ç›´æ¥é€šä¿¡ï¼Œå¸¸ç”¨äºèŠ‚ç‚¹å†…é€šä¿¡
            - InfiniBand: æœ¬è´¨ä¸Šä¹Ÿæ˜¯ç¡¬ä»¶ï¼Œå¸¸ç”¨äºèŠ‚ç‚¹é—´é€šä¿¡
            - RDMA: RDMAç½‘å¡ï¼Œæœ€å¤§çš„å¥½å¤„æ˜¯è·³è¿‡æ“ä½œç³»ç»Ÿ / zero copy, RoCE
        - é€šä¿¡åº“ï¼š'vllm/distributed/device_communicators'
            - PyNccl: Nvidia ä¹‹é—´çš„é€šä¿¡
            - Shared memory: æ“ä½œç³»ç»Ÿä¸­ä¸åŒè¿›ç¨‹ä¹‹é—´æ•°æ®å…±äº«
            - Custom allreduce: ä¸“ä¸ºall reduceæ“ä½œçš„kernel 
            - torch.distributed: å¹¿æ³›æ”¯æŒä¸€ç³»åˆ—çš„é€šä¿¡åº“
    - ç®—æ³•ç«¯
        - æƒ³äº†è§£ä»»ä½•é€šä¿¡æ–¹å¼å¯ä»¥äº†è§£ï¼š'vllm/model_executor/models/llama.py'ï¼Œllamaç³»æ¨¡å‹æ”¯æŒå„ç§å¹¶è¡Œæ–¹å¼ï¼Œé€‚åˆåˆå­¦è€…å­¦ä¹ æ¶æ„
        - 'get_tp_group()'

- PPï¼ˆæµæ°´çº¿å¹¶è¡Œï¼‰
    - é€šä¿¡é‡ç›¸å¯¹è¾ƒå°ï¼Œå¯¹device--deviceé€šä¿¡éœ€æ±‚è¾ƒä½
    - ä¸èƒ½é™ä½å»¶è¿Ÿï¼Œä½†èƒ½æé«˜åå
    - ç®—æ³•ç«¯
        - æ¯ä¸ªworkerè´Ÿè´£ä¸€ä¸ªlayersçš„å­é›†
            - 'vllm/model_executor/models/llama.py' ä¸­ self.start_layer --> self.end_layer
            - åœ¨workerä¹‹é—´: communicate IntermediateTensor
            - 'vllm/worker/model_runner.py': æœç´¢ 'get_pp_group()'

- EPï¼ˆä¸“å®¶å¹¶è¡Œï¼‰& DPï¼ˆæ•°æ®å¹¶è¡Œï¼‰
    - ä¸ºä»€ä¹ˆè¦æœ‰EPï¼Ÿ
        - Mistral / Mixtral / Deepseek éƒ½æ˜¯ç”¨MOE
        - MOEå…·æœ‰è®¡ç®—ç¨€ç–æ€§ï¼Œæ¯ä¸ªrequeståªæ¿€æ´»ä¸€å°éƒ¨åˆ†çš„expert
    - å°†ä¸åŒçš„expertæ”¾åœ¨ä¸åŒçš„deviceä¸Š-->ä¸“å®¶å¹¶è¡Œ
    - ç®—æ³•ç«¯
        - Shuffle (DeepEP communication kernel)
        - Forward
        - Shuffle back
    - åœ¨Attentionæ¨¡å—åšTPï¼Œåœ¨FFNæ¨¡å—åšEP
    - share expertè´Ÿè½½è¾ƒé«˜ï¼Œè¦åšå†—ä½™

    - DP (æ•°æ®å¹¶è¡Œ)
        - æœ€å¤§çš„TP << æ‰€éœ€è¦çš„EPï¼ˆEP=320ï¼‰
        - TP < # attention head
        - TP * DP == EPï¼ˆé€šè¿‡è¯·æ±‚å¹¶è¡Œçš„æ–¹å¼å»æ‹‰æ»¡è®¡ç®—èµ„æºï¼‰
        - åœ¨å®è·µä¸­éš¾ä»¥åº”ç”¨
            - å¯¹è¯·æ±‚è¿›è¡Œpaddingé¿å…é€ æˆæ­»é”


â€‹    